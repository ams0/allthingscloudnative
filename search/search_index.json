{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to All Tings Cloud Native Workshop","text":"<p>This is the companion hands-on workshop for the All Things Cloud Native presentation.  The workshop is designed to be self-paced and self-guided.  The workshop is broken into multiple sections, with navigation links at the top of each page.  The workshop is designed to be completed in order, but you can jump around if you like.</p> <p>Slides are here.</p>"},{"location":"#sections","title":"Sections","text":"<ul> <li><code>Containers</code> - All things about Docker/Podman containers</li> <li><code>Kubernetes</code> - The core Kubernetes concepts</li> <li><code>Helm</code> - Package manager for Kubernetes</li> <li><code>Observability</code> - Observability and telemetry related workshops.</li> <li><code>Azure Kubernetes</code> - For the Azure Managed Kubernetes Service.</li> <li><code>Openshift</code> - All the Openshift-related material.</li> <li><code>References</code> - Miscellaneus.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"aks/","title":"Azure Kubernetes Service (AKS)","text":"<p>This is the AKS module of the workshop. It is intended to be used as a reference for the instructor to guide the students through the lab.</p>"},{"location":"containers/","title":"Containers","text":"<p>Getting started with containers is easy! Follow along and ask questions, we're here to help.</p>"},{"location":"containers/#outcomes","title":"Outcomes","text":"<p>You should be able to:</p> <ul> <li>Understand the install and use a docker runtime and client</li> <li>Containerize an application</li> </ul>"},{"location":"containers/#outline","title":"Outline","text":"<ol> <li> <p>Install Docker</p> <p>The best way, on Linux nodes, is to use the script:</p> <pre><code>curl -fsSL get.docker.com -o get-docker.sh &amp;&amp; sudo sh get-docker.sh &amp;&amp; sudo systemctl start docker &amp;&amp; sudo systemctl enable docker\n</code></pre> <p>Don't forget to add your user to the docker group:</p> <pre><code>sudo usermod -aG docker $USER\n</code></pre> <p>You'll need to logout and login again for the group change to take effect.</p> </li> <li> <p>Run a container</p> <pre><code>docker run busybox echo Hello World\n</code></pre> <p>What just happened?</p> <ol> <li>The docker client contacted the docker daemon process</li> <li>The docker daemon pulled the \"busybox\" image from the docker hub</li> <li>The docker daemon created a new container from that image which runs the executable that produces the output you are currently reading</li> <li>The docker daemon streamed that output to the docker client, which sent it to your terminal</li> <li>The docker daemon then exited</li> <li>The docker client exited</li> <li>The container was destroyed</li> </ol> </li> <li> <p>Run a container in the background</p> <pre><code>docker run -d -p 80:80 nginx\n</code></pre> <p>Connect to the nginx container:</p> <pre><code>curl localhost\n</code></pre> </li> <li> <p>Run a container in the background</p> </li> </ol> <pre><code>docker run -d -p 80:80 nginx\n</code></pre>"},{"location":"containers/container-networking/","title":"Container Networking","text":""},{"location":"containers/container-networking/#outcomes","title":"Outcomes","text":"<p>You should be able to:</p> <ul> <li>Understand docker networking</li> </ul>"},{"location":"containers/container-networking/#outline","title":"Outline","text":"<ol> <li>Docker networking</li> </ol> <pre><code>docker run \u2010\u2010net=none busybox ifconfig\nlo        Link encap:Local Loopback\n          inet addr:127.0.0.1  Mask:255.0.0.0\n          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n          collisions:0 txqueuelen:1\n          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\n</code></pre> <pre><code>docker run --network host busybox ifconfig\n</code></pre> <p>Two containers can share the same network:</p> <pre><code>docker run \u2010\u2010name foo \u2010d busybox sleep 3600\ndocker run \u2010\u2010net=container:foo \u2010it busybox /bin/sh\n</code></pre>"},{"location":"containers/container-registries/","title":"Explore and interact with registries","text":""},{"location":"containers/container-registries/#outcomes","title":"Outcomes","text":"<ul> <li>Understand the difference between a registry and a repository</li> <li>Understand how to interact with a registry</li> </ul>"},{"location":"containers/container-registries/#outline","title":"Outline","text":"<p>(the below can be done with <code>podman</code> or <code>docker</code>)</p> <p>Search a registry for an image</p> <pre><code>podman search registry.access.redhat.com &lt;pattern&gt;\n</code></pre> <p>Pull an image from a registry</p> <pre><code>podman pull registry.access.redhat.com/rhel7/rhel\n</code></pre> <p>Tag images</p> <pre><code>podman tag registry.access.redhat.com/rhel7/rhel rhel7/rhel\n</code></pre> <p>Push images to a registry</p> <pre><code>podman login -u &lt;username&gt; -p &lt;password&gt; &lt;registry&gt;:&lt;port&gt;\npodman push rhel7/rhel &lt;registry&gt;:&lt;port&gt;/&lt;username&gt;/rhel7/rhel\n</code></pre>"},{"location":"containers/container-security/","title":"Containers Security","text":"<p>See Pod security  section.</p>"},{"location":"containers/create-app/","title":"Create an application and containerize it","text":""},{"location":"containers/create-app/#outcomes","title":"Outcomes","text":"<p>You should be able to:</p> <ul> <li>Containerize a node.js application</li> </ul>"},{"location":"containers/create-app/#outline","title":"Outline","text":""},{"location":"containers/create-app/#nodejs","title":"Nodejs","text":"<ol> <li>Create a node.js app:</li> </ol> <pre><code>mkdir temp-node\ncd temp-node\nnpm init -y\nnpm install express\n\necho &lt;&lt;EOF &gt;app.js\n// app.js\n\nconst express = require('express');\nconst app = express();\nconst port = 3000;\n\napp.get('/', (req, res) =&gt; {\n  res.send('Hello World!');\n});\n\napp.listen(port, () =&gt; {\n  console.log(`App listening at http://localhost:${port}`);\n});\nEOF\n</code></pre> <ol> <li>Create a Dockerfile</li> </ol> <pre><code>FROM node:16\n\n# Set the working directory inside the container\nWORKDIR /usr/src/app\n\n# Copy package.json and package-lock.json (if available) to the working directory\nCOPY package*.json ./\n\n# Install dependencies\nRUN npm install\n\n# Copy the rest of the application files to the working directory\nCOPY . .\n\n# Expose port 3000\nEXPOSE 3000\n\n# Start the application\nCMD [\"node\", \"app.js\"]\n</code></pre> <ol> <li>Build the image</li> </ol> <pre><code>docker build -t my-simple-node-app .\n</code></pre> <pre><code>List the image:\n</code></pre> <pre><code>docker image list\n</code></pre> <ol> <li>Run the image as a container</li> </ol> <pre><code>docker run -p 3000:3000 my-simple-node-app```\n\n5. Connect to the app\n\nOpen a browser to http://localhost:3000. Success!\n\nCheck the running containers:\n\n```bash\ndocker container list\n</code></pre> <ol> <li>Create a Go app and containerize it</li> </ol> <p>Create a file with the following content:</p> <pre><code>package main\n\nimport \"fmt\"\n\nfunc main() {\n    fmt.Println(\"hello world\")\n}\n</code></pre> <p>Now you have two options:</p> <ul> <li>compile the app and run the binary in a docker container:</li> </ul> <pre><code>GOOS=linux GOARCH=amd64 go build -o hello\n</code></pre> <ul> <li>run the app in a docker container without compiling it:</li> </ul> <pre><code>docker run -it --rm -v \"$PWD\":/usr/src/myapp -w /usr/src/myapp golang:1.14 go run hello.go\n</code></pre> <p>Create a basic dockerfile that does the same:</p> <pre><code>FROM scratch\n\nCOPY hello /\n\nENTRYPOINT [\"/hello\"]\n</code></pre> <pre><code>docker build -t hello:scratch .\ndocker run hello:scratch\n</code></pre> <ul> <li>Without installing any local dependencies, build the app and the image::</li> </ul> <pre><code>FROM golang:1.14 AS builder\nCOPY hello.go .\nRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o hello .\n\nFROM scratch\nCOPY --from=builder /go/hello /hello\nENTRYPOINT [\"/hello\"]\n</code></pre> <pre><code>docker build -t hello:multibuild .\n</code></pre> <p>Run the container with:</p> <pre><code>docker run hello:multibuild\n</code></pre>"},{"location":"containers/create-app/#python","title":"Python","text":"<ol> <li>Create a Python app</li> </ol> <p>This Flask application listens on port 5000 and returns a simple \"Hello, Docker!\" message when accessed.</p> <ul> <li> <p>Python script (app.py) <pre><code># app.py\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello_world():\n    return 'Hello, Docker!'\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=5000)\n</code></pre></p> </li> <li> <p>requirements.txt  This file lists the Python dependencies needed for the app. In this case, you only need Flask and Werkzeug:</p> </li> </ul> <p><code>makefile  Werkzeug==2.2.2  Flask==2.0.2</code></p> <ol> <li>Python Dockerfile</li> </ol> <pre><code># Use an official Python runtime as a base image\nFROM python:3.9-slim\n# Set the working directory\nWORKDIR /app\n# Copy the current directory contents into the container\nCOPY . /app\n# Install any needed dependencies\nRUN pip install -r requirements.txt\n# Make port 5000 available to the world outside this container\nEXPOSE 5000\n# Run app.py when the container launches\nCMD [\"python\", \"app.py\"]\n    ```\n\n2. Build the image\n\n```bash\ndocker build -t my-python-app .\n</code></pre> <ol> <li>Run the image as a container</li> </ol> <pre><code>docker run -p 5000:5000 my-python-app\n</code></pre>"},{"location":"containers/create-app/#java","title":"Java","text":"<ol> <li>Create a Java app</li> </ol> <p>You can generate a Spring Boot application using Spring Initializr. Here are the basic steps:</p> <ul> <li>Project: Maven</li> <li>Language: Java</li> <li>Spring Boot: 2.7.x or later</li> <li>Dependencies: Spring Web</li> </ul> <p>Once you generate the project, you will get a Maven project structure. Add the following simple Java code to your <code>Application.java</code> file.</p> <pre><code>package com.example.myapp;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.RestController;\n\n@SpringBootApplication\npublic class MyAppApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(MyAppApplication.class, args);\n    }\n\n    @RestController\n    class HelloController {\n        @GetMapping(\"/\")\n        public String hello() {\n            return \"Hello, Docker!\";\n        }\n    }\n}\n</code></pre> <p>Build the Java Application Once you have the code and <code>pom.xml</code> in place, you need to build the project to generate the JAR file. Run the following Maven command from the root directory of your project:</p> <p><pre><code>mvn clean package\n</code></pre> This will generate a JAR file in the <code>target/</code> directory, e.g., <code>target/myapp-0.0.1-SNAPSHOT.jar</code>.</p> <ol> <li>Java Dockerfile</li> </ol> <pre><code># Use an official Java runtime as a base image\nFROM openjdk:11-jre-slim\n# Copy the jar file into the container\nCOPY target/myapp.jar /app/myapp.jar\n# Expose port 8080\nEXPOSE 8080\n# Run the jar file when the container launches\nCMD [\"java\", \"-jar\", \"/app/myapp.jar\"]\n</code></pre> <ol> <li>Build the image</li> </ol> <pre><code>docker build -t my-java-app .\n</code></pre> <ol> <li>Run the image as a container</li> </ol> <pre><code>docker run -p 8080:8080 my-java-app\n</code></pre>"},{"location":"containers/docker-compose/","title":"Docker Compose","text":""},{"location":"containers/docker-compose/#outcomes","title":"Outcomes","text":"<ul> <li>Understand what Docker Compose is.</li> <li>Understand how to use Docker Compose.</li> <li>Understand how to use Docker Compose with a Dockerfile.</li> <li>Understand how to use Docker Compose with a Dockerfile and a <code>.env</code> file.</li> </ul>"},{"location":"containers/docker-compose/#what-is-docker-compose","title":"What is Docker Compose?","text":"<p>Docker Compose is a tool used for defining and running multi-container Docker applications. It allows you to configure your application\u2019s services using a simple YAML file, which makes it easier to start, stop, and manage all of the services that comprise your application.</p> <p>In essence, Docker Compose helps manage complex environments by simplifying multi-container setups. With Compose, you can: - Define services (such as databases, web servers, etc.) in a single file. - Spin up all defined services with a single command (<code>docker-compose up</code>). - Scale services as needed (e.g., run multiple instances of a web application).</p>"},{"location":"containers/docker-compose/#key-benefits-of-docker-compose","title":"Key Benefits of Docker Compose:","text":"<ul> <li>Simplified multi-container management: Compose orchestrates the entire environment of containers.</li> <li>Declarative syntax: The use of YAML configuration files makes defining services clear and concise.</li> <li>Reusability: Compose files can be shared or reused across environments like development, testing, and production.</li> </ul>"},{"location":"containers/docker-compose/#how-to-use-docker-compose","title":"How to Use Docker Compose","text":""},{"location":"containers/docker-compose/#step-1-define-your-services-in-a-docker-composeyml-file","title":"Step 1: Define Your Services in a <code>docker-compose.yml</code> File","text":"<p>In the <code>docker-compose.yml</code> file, you define the services that make up your application. For example, a typical multi-service app might have a web server and a database. Here's a basic example:</p> <pre><code>version: '3'\nservices:\n  web:\n    image: nginx\n    ports:\n      - \"80:80\"\n  db:\n    image: postgres\n    environment:\n      POSTGRES_PASSWORD: example\n</code></pre> <p>In this file:</p> <p>We define two services: <code>web</code> and <code>db</code>. The <code>web</code> service uses the nginx image and maps port 80 on the container to port 80 on the host. The <code>db</code> service uses the postgres image and sets an environment variable for the database password.</p>"},{"location":"containers/docker-compose/#step-2-starting-services","title":"Step 2: Starting Services","text":"<p>Once the <code>docker-compose.yml</code> file is defined, you can start your application using the following command:</p> <pre><code>docker-compose up\n</code></pre> <p>This will start all the services defined in your Compose file. To stop the services, you can use:</p> <pre><code>docker-compose down\n</code></pre>"},{"location":"containers/docker-compose/#step-3-scaling-services","title":"Step 3: Scaling Services","text":"<p>To scale a service (for example, to run multiple instances of the web service), you can use:</p> <pre><code>docker-compose up --scale web=3\n</code></pre> <p>This command will create three instances of the web service.</p>"},{"location":"containers/docker-compose/#using-docker-compose-with-a-dockerfile","title":"Using Docker Compose with a Dockerfile","text":"<p>You can instruct Docker Compose to build an image from a Dockerfile instead of pulling it from a registry like Docker Hub. This is especially useful when you need to customize your container images.</p> <p>Example: docker-compose.yml with a Dockerfile <pre><code>version: '3'\nservices:\n  web:\n    build: .\n    ports:\n      - \"80:80\"\n  db:\n    image: postgres\n    environment:\n      POSTGRES_PASSWORD: example\n</code></pre></p> <p>In this case:</p> <ul> <li>The <code>web</code> service uses a Dockerfile located in the same directory as the <code>docker-compose.yml</code> file to build its image.</li> <li>The <code>db</code> service still uses the postgres image from Docker Hub. You would need a Dockerfile in your project directory, like this:</li> </ul> <pre><code># Dockerfile\nFROM nginx\nCOPY ./html /usr/share/nginx/html\n</code></pre> <p>When you run <code>docker-compose up</code>, it will automatically build the web service image from the Dockerfile.</p>"},{"location":"containers/docker-compose/#using-docker-compose-with-a-dockerfile-and-a-env-file","title":"Using Docker Compose with a Dockerfile and a .env File","text":"<p>You can further customize your Docker Compose environment by using a .env file. This file contains environment variables that you can reference in the <code>docker-compose.yml</code> file.</p> <p>Example: .env File <pre><code>POSTGRES_PASSWORD=supersecretpassword\n</code></pre></p> <p>Example: docker-compose.yml Using .env</p> <pre><code>version: '3'\nservices:\n  db:\n    image: postgres\n    environment:\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n</code></pre> <p>In this example:</p> <ul> <li>The password for the db service is read from the .env file.</li> <li>You can use environment variables in your <code>docker-compose.yml</code> file by referring to them with <code>${VARIABLE_NAME}</code>.</li> </ul>"},{"location":"containers/docker-compose/#summary","title":"Summary","text":"<p>Docker Compose simplifies the process of managing multi-container Docker applications. With a single <code>docker-compose.yml</code> file, you can define multiple services, build images using Dockerfiles, and pass environment variables through .env files. These features make Compose an essential tool for orchestrating and managing containerized applications.</p>"},{"location":"containers/layers/","title":"OCI image layers","text":"<p>TBD</p>"},{"location":"containers/lifecycle/","title":"Container lifecycle","text":""},{"location":"containers/lifecycle/#outcomes","title":"Outcomes","text":"<p>You should be able to: - Understand the lifecycle of a container</p>"},{"location":"containers/lifecycle/#outline","title":"Outline","text":"<p>(the below can be done with docker or podman)</p> <ol> <li>Create a container but don't run it:</li> </ol> <pre><code>docker create -p 80:80 --name web_server drupal\n</code></pre> <ol> <li>Start the container:</li> </ol> <pre><code>docker start web_server\n</code></pre> <p>The above two commands can be combined into one:</p> <pre><code>docker run -p 80:80 --name web_server drupal\n</code></pre> <ol> <li>Pause the container:</li> </ol> <pre><code>docker pause web_server\n</code></pre> <ol> <li>Unpause the container:</li> </ol> <pre><code>docker unpause web_server\n</code></pre> <ol> <li>Stop the container:</li> </ol> <pre><code>docker stop web_server\n</code></pre> <ol> <li>Inspect the container</li> </ol> <pre><code>docker inspect web_server\n</code></pre> <ol> <li>Remove the container:</li> </ol> <pre><code>docker rm web_server\ndocker ps -a\n</code></pre>"},{"location":"containers/local-registries/","title":"Run a local registry","text":""},{"location":"containers/local-registries/#outcomes","title":"Outcomes","text":"<ul> <li>Run a local registry to push and pull images from</li> <li>Understand how to interact with a local registry</li> </ul>"},{"location":"containers/local-registries/#outline","title":"Outline","text":"<p>Run a local registry:</p> <pre><code>docker run -d -p 5000:5000 --name registry registry:latest\n</code></pre> <p>Pull, tag and push to local registry:</p> <pre><code>docker pull ubuntu\ndocker tag ubuntu localhost:5000/ubuntu\ndocker push localhost:5000/ubuntu\n</code></pre> <p>Use crane to inspect the registry:</p> <pre><code>#install crane\nVERSION=$(curl -s \"https://api.github.com/repos/google/go-containerregistry/releases/latest\" | jq -r '.tag_name')\nARCH=x86_64 \nOS=Linux \ncurl -sL \"https://github.com/google/go-containerregistry/releases/download/${VERSION}/go-containerregistry_${OS}_${ARCH}.tar.gz\" &gt; go-containerregistry.tar.gz\ntar -zxvf go-containerregistry.tar.gz -C /usr/local/bin/ crane\n\n\ncrane catalog localhost:5005\nubuntu\n</code></pre>"},{"location":"containers/podman/","title":"Use podman to run containers","text":""},{"location":"containers/podman/#outcomes","title":"Outcomes","text":"<p>You should be able to: - Understand how to run containers with podman - Understand how to run containers with podman-compose - Understand how to run containers with podman-pod</p>"},{"location":"containers/podman/#outline","title":"Outline","text":"<ol> <li>Run a container with podman using the <code>ubi</code> image (the <code>ubi</code> image is a minimal image based on Red Hat Universal Base Image)</li> </ol> <pre><code>podman run --rm -it ubi bash\n</code></pre> <p>Notice the use of the <code>--rm</code> flag. This flag will remove the container once it exits. Also, note the use of the <code>-it</code> flags. These flags are used to run the container interactively and to allocate a pseudo-TTY.</p>"},{"location":"containers/privileged-containers/","title":"Privileged Containers","text":""},{"location":"containers/privileged-containers/#outcomes","title":"Outcomes","text":"<p>You should be able to:</p> <ul> <li>Understand how to limit container capabilities</li> </ul>"},{"location":"containers/privileged-containers/#outline","title":"Outline","text":"<p>Create a container that drops the CHOWN capability:</p> <pre><code>$ docker run \u2010\u2010rm \u2010it \u2010\u2010cap\u2010drop=CHOWN busybox chown \u2010R nobody:\nchown: /bin/true: Operation not permitted\nchown: /bin/getty: Operation not permitted\nchown: /bin/hd: Operation not permitted\nchown: /bin/mknod: Operation not permitted\n</code></pre> <p>You can drop all caps and add them back in one by one: </p> <pre><code>docker run \u2010d \u2010\u2010cap\u2010drop=all\n              \u2010\u2010cap\u2010add=setuid\n              \u2010\u2010cap\u2010add=setgid busybox top\n</code></pre> <p>Note</p> <p>Privliedged containers are not recommended for production use.</p>"},{"location":"containers/volumes/","title":"Persistent Volumes in Docker/Podman","text":""},{"location":"containers/volumes/#outcomes","title":"Outcomes","text":"<ul> <li>Understand the difference between a volume and a bind mount</li> <li>Understand how to create a volume</li> </ul>"},{"location":"containers/volumes/#outline","title":"Outline","text":"<p>The \"old\" docker style:</p> <pre><code>docker run -v /path/to/the/volume:/path/in/the/container/fs \\\n        [&lt;options&gt;...] &lt;image&gt; [&lt;command&gt;]\n</code></pre> <p>the new podman style:</p> <pre><code>podman run  --name mycontainer \\\n        --mount type=bind,src=/path/on/host,dst=/path/in/container \\\n        [&lt;options&gt;...] &lt;image&gt; [&lt;command&gt;]\n</code></pre>"},{"location":"containers/volumes/#exercise","title":"Exercise","text":"<pre><code>LOG_SRC=~/lab.log\nLOG_DST=/var/log/dpkg.log\npodman run -d --name bind-container --mount type=bind,src=$LOG_SRC,dst=$LOG_DST -p 80:80 drupal\n</code></pre> <p>We create some logs</p> <pre><code>podman exec -it bind-container bash\necho \"This is a log from within the container\" &gt;&gt; /var/log/dpkg.log\nexit\n\ncat lab.log\n</code></pre>"},{"location":"containers/volumes/#volumes","title":"Volumes","text":"<pre><code>podman volume create volume1\npodman run -d --name volume-container --mount type=volume,src=volume1,dst=/var/log -p 80:80 drupal\npodman inspect volume-container\n</code></pre>"},{"location":"containers/working-with-images/","title":"Working with images","text":""},{"location":"containers/working-with-images/#outcomes","title":"Outcomes","text":"<p>You should be able to:</p> <ul> <li>Understand how images are built and created</li> </ul>"},{"location":"containers/working-with-images/#outline","title":"Outline","text":"<ol> <li> <p>Create a CentOS image from scratch:</p> <pre><code>FROM scratch\nADD centos\u20107\u2010docker.tar.xz /\nLABEL org.label\u2010schema.schema\u2010version=\"1.0\" \\\n    org.label\u2010schema.name=\"CentOS Base Image\" \\\n    org.label\u2010schema.vendor=\"CentOS\" \\\n    org.label\u2010schema.license=\"GPLv2\" \\\n    org.label\u2010schema.build\u2010date=\"20180804\"\nCMD [\"/bin/bash\"]\n</code></pre> </li> <li> <p>Build the image</p> <pre><code>docker build --tag centos:7 .\n</code></pre> </li> <li> <p>Run the image</p> <pre><code>docker run -ti centos:7\n</code></pre> </li> </ol>"},{"location":"helm/","title":"Helm","text":""},{"location":"helm/#resources","title":"Resources","text":"<p>Helm Playground</p>"},{"location":"helm/#outcomes","title":"Outcomes","text":"<ul> <li>Understand what Helm is and its role in Kubernetes.</li> <li>Learn how to install and configure Helm in a Kubernetes environment.</li> <li>Understand how to use Helm charts to deploy applications.</li> <li>Learn how to create and manage custom Helm charts.</li> <li>Understand how to use Helm for application upgrades and rollbacks.</li> </ul>"},{"location":"helm/#what-is-helm","title":"What is Helm?","text":"<p>Helm is a package manager for Kubernetes that simplifies the process of deploying, managing, and upgrading Kubernetes applications. Helm uses charts, which are pre-configured Kubernetes resource templates, to define the configuration and deployment of an application or service.</p>"},{"location":"helm/#key-benefits-of-helm","title":"Key Benefits of Helm:","text":"<ul> <li>Simplified deployments: Helm automates the deployment of Kubernetes applications by using pre-configured templates.</li> <li>Version control: Helm allows versioning of charts and rollback capabilities in case of failures.</li> <li>Reuse: Helm charts can be reused across different environments such as development, staging, and production.</li> <li>Customization: Charts can be customized using values, allowing flexibility without modifying the core chart.</li> </ul>"},{"location":"helm/#how-to-install-helm","title":"How to Install Helm","text":"<p>To use Helm in your Kubernetes cluster, you first need to install Helm on your local machine and configure it to interact with your Kubernetes cluster.</p>"},{"location":"helm/#step-1-install-helm-cli","title":"Step 1: Install Helm CLI","text":"<p>The Helm client can be installed using the following commands depending on your operating system:</p> <ul> <li> <p>macOS:   <pre><code>brew install helm\n</code></pre></p> </li> <li> <p>Linux:   <pre><code>curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash\n</code></pre></p> </li> <li> <p>Windows:   Download the latest release from the Helm GitHub releases page.</p> </li> </ul>"},{"location":"helm/#step-2-configure-helm","title":"Step 2: Configure Helm","text":"<p>Once Helm is installed, configure it to use your Kubernetes cluster by ensuring that <code>kubectl</code> is configured and connected to your cluster. Helm interacts with the Kubernetes API server using <code>kubectl</code>.</p>"},{"location":"helm/#step-3-adding-a-helm-repository","title":"Step 3: Adding a Helm Repository","text":"<p>Helm uses repositories to store and distribute charts. To add the official Helm stable repository, use the following command:</p> <pre><code>helm repo add stable https://charts.helm.sh/stable\n</code></pre> <p>To list all available repositories:</p> <pre><code>helm repo list\n</code></pre>"},{"location":"helm/#using-helm-charts-to-deploy-applications","title":"Using Helm Charts to Deploy Applications","text":"<p>Helm charts define the structure of a Kubernetes application. A chart contains templates for Kubernetes resources, including deployments, services, and config maps.</p>"},{"location":"helm/#step-1-searching-for-a-chart","title":"Step 1: Searching for a Chart","text":"<p>Helm allows you to search for charts in a repository. For example, to search for <code>nginx</code> charts in the stable repository:</p> <pre><code>helm search repo nginx\n</code></pre>"},{"location":"helm/#step-2-installing-a-helm-chart","title":"Step 2: Installing a Helm Chart","text":"<p>To deploy an application using Helm, you can use the <code>helm install</code> command. For example, to install an <code>nginx</code> server:</p> <pre><code>helm install my-nginx stable/nginx\n</code></pre> <p>This command will deploy the <code>nginx</code> chart, and Helm will create the necessary Kubernetes resources based on the chart's configuration.</p>"},{"location":"helm/#step-3-viewing-the-deployed-application","title":"Step 3: Viewing the Deployed Application","text":"<p>You can view the status of the deployed application using the <code>helm status</code> command:</p> <pre><code>helm status my-nginx\n</code></pre> <p>To see all installed Helm releases:</p> <pre><code>helm list\n</code></pre>"},{"location":"helm/#customizing-helm-charts","title":"Customizing Helm Charts","text":"<p>Helm allows you to customize the deployment of applications by passing a <code>values.yaml</code> file or using the <code>--set</code> flag during installation. </p>"},{"location":"helm/#example-customizing-a-deployment","title":"Example: Customizing a Deployment","text":"<p>You can pass custom values when installing a chart using the <code>--set</code> option. For example, to customize the <code>replicaCount</code> value for an <code>nginx</code> deployment:</p> <pre><code>helm install my-nginx stable/nginx --set replicaCount=3\n</code></pre> <p>Alternatively, you can create a <code>values.yaml</code> file with your custom values:</p> <pre><code>replicaCount: 3\n</code></pre> <p>Then, install the chart with the values file:</p> <pre><code>helm install my-nginx stable/nginx -f values.yaml\n</code></pre>"},{"location":"helm/#creating-custom-helm-charts","title":"Creating Custom Helm Charts","text":"<p>In addition to using pre-existing charts, Helm allows you to create custom charts for your own applications.</p>"},{"location":"helm/#step-1-create-a-new-helm-chart","title":"Step 1: Create a New Helm Chart","text":"<p>You can create a new Helm chart using the following command:</p> <pre><code>helm create my-chart\n</code></pre> <p>This command generates a directory structure for your new chart, which includes: - <code>templates/</code>: Contains the Kubernetes manifest templates. - <code>values.yaml</code>: Default configuration values. - <code>Chart.yaml</code>: Metadata about the chart (name, version, etc.).</p>"},{"location":"helm/#step-2-editing-the-chart","title":"Step 2: Editing the Chart","text":"<p>You can modify the default chart to fit your application. For example, you can edit the deployment template in the <code>templates/deployment.yaml</code> file.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: {{ include \"my-chart.fullname\" . }}\nspec:\n  replicas: {{ .Values.replicaCount }}\n  template:\n    metadata:\n      labels:\n        app: {{ include \"my-chart.name\" . }}\n    spec:\n      containers:\n      - name: {{ .Chart.Name }}\n        image: {{ .Values.image.repository }}:{{ .Values.image.tag }}\n        ports:\n        - containerPort: 80\n</code></pre>"},{"location":"helm/#step-3-deploying-the-custom-chart","title":"Step 3: Deploying the Custom Chart","text":"<p>Once your custom chart is ready, you can deploy it using Helm:</p> <pre><code>helm install my-app ./my-chart\n</code></pre> <p>Helm will render the templates with the provided values and create the Kubernetes resources.</p>"},{"location":"helm/#helm-chart-upgrades-and-rollbacks","title":"Helm Chart Upgrades and Rollbacks","text":"<p>Helm simplifies application management by allowing seamless upgrades and rollbacks.</p>"},{"location":"helm/#step-1-upgrading-a-helm-release","title":"Step 1: Upgrading a Helm Release","text":"<p>To upgrade an existing Helm release with new values or a newer version of the chart:</p> <pre><code>helm upgrade my-nginx stable/nginx --set replicaCount=5\n</code></pre> <p>Helm will apply the changes to the Kubernetes resources without downtime.</p>"},{"location":"helm/#step-2-rolling-back-a-helm-release","title":"Step 2: Rolling Back a Helm Release","text":"<p>If something goes wrong after an upgrade, Helm allows you to roll back to a previous version:</p> <pre><code>helm rollback my-nginx 1\n</code></pre> <p>This command will roll back the release <code>my-nginx</code> to revision 1.</p>"},{"location":"helm/#best-practices-for-using-helm","title":"Best Practices for Using Helm","text":"<ol> <li>Version your charts: Always version your custom charts to track changes over time.</li> <li>Test charts locally: Use the <code>helm template</code> command to render templates locally and check for errors before deploying.</li> <li>Use values.yaml for configuration: Store custom values in a <code>values.yaml</code> file instead of using the <code>--set</code> flag to keep deployments consistent.</li> <li>Automate with CI/CD: Use Helm in your CI/CD pipeline to automate deployments and manage upgrades in a consistent and repeatable way.</li> <li>Secure Helm: Helm 3 removed Tiller, improving security, but ensure your Helm charts and deployments follow best security practices, such as least privilege and encryption for sensitive data.</li> </ol>"},{"location":"helm/#summary","title":"Summary","text":"<p>Helm simplifies Kubernetes application management by packaging, deploying, and upgrading applications using charts. With Helm, you can easily deploy complex applications, manage configuration through <code>values.yaml</code> files, and handle versioning and rollbacks. By using Helm, Kubernetes users can streamline their deployment processes, improve application scalability, and automate infrastructure management. ```</p>"},{"location":"kubernetes/","title":"Getting started with Kubernetes","text":""},{"location":"kubernetes/#outcomes","title":"Outcomes","text":"<p>You should be able to:</p> <ul> <li>Understand the install and use a Kubernetes runtime and client</li> <li>Deploy a simple application</li> <li>Understand how to scale an application</li> </ul>"},{"location":"kubernetes/#outline","title":"Outline","text":"<ol> <li>Run Kubernetes</li> </ol> <p>You can choose your tools but we recommend either minikube, kind or microk8s</p> <pre><code>minikube start\n</code></pre> <p>or more fancy:</p> <pre><code>minikube start --kubernetes-version=v1.28.0-rc.1 --nodes 3\n</code></pre> <p>Enable the dashboard and the metrics server:</p> <pre><code>minikube addons enable metrics-server\nminikube addons enable dashboard\nminikube dashboard\n</code></pre> <p>Go ahead and deploy a <code>redis</code> image using the UI. Access Redis with:</p> <pre><code>$ kubectl get pods\n$ kubectl logs redis\u20105o0i8\n$ kubectl exec \u2010ti redis\u20105o0i8 bash root@redis\u20105o0i8:/data# redis\u2010cli 127.0.0.1:6379&gt;\n</code></pre> <p>You can use <code>minikube ssh</code> to poke around the nodes; check which node the pod is running on:</p> <pre><code>$ kubectl get pods \u2010o wide\n</code></pre> <p>Explore <code>kubectl</code> commands:</p> <pre><code>$ kubectl explain pods\n$ kubectl explain pods.metadata\n$ kubectl describe pods redis\u20107f5f77dc44\u2010mpjvc\n</code></pre>"},{"location":"kubernetes/api/","title":"Explore the Kubernetes API","text":""},{"location":"kubernetes/api/#outcomes","title":"Outcomes","text":"<ul> <li>Understand the Kubernetes API</li> <li>Query the Kubernetes API using kubectl and curl</li> </ul>"},{"location":"kubernetes/api/#outline","title":"Outline","text":"<p>Increase verbosity of kubectl output</p> <pre><code>kubectl get pods -v=8\n</code></pre> <p>Notice the <code>curl</code> command at the top of the output; save it as you'll need it later.</p> <p>Create a service account and cluster role binding</p> <pre><code>kubectl create serviceaccount k8s-api-explorer\nkubectl create clusterrolebinding k8s-api-explorer --clusterrole=cluster-admin --serviceaccount=default:k8s-api-explorer\n</code></pre> <p>Get the token for the service account</p> <pre><code>TOKEN=$(kubectl create token k8s-api-explorer -o jsonpath='{.data.token}')\n</code></pre> <p>Now construct the curl command to query the API</p> <pre><code>curl -k -v -XGET -H \"Authorization: Bearer ${TOKEN}\" &lt;URL from command above&gt;\n</code></pre>"},{"location":"kubernetes/cli/","title":"CLIs","text":""},{"location":"kubernetes/cli/#autocompletion","title":"Autocompletion","text":"<p>Follow the instructions for your OS/Shell:</p> <pre><code>#for linux\necho 'source &lt;(kubectl completion bash)' &gt;&gt;~/.bashrc\nsource &lt;(kubectl completion bash) # to have it in this session\n</code></pre> <p>Press tab after <code>kubectl</code> to see the list of commands, or listing the resources available</p>"},{"location":"kubernetes/cluster/","title":"Explore the cluster","text":""},{"location":"kubernetes/cluster/#outcomes","title":"Outcomes","text":"<p>You should be able to:</p> <ul> <li>Get an idea on how Kubernetes works</li> <li>Understand the basic concepts of Kubernetes</li> </ul>"},{"location":"kubernetes/cluster/#outline","title":"Outline","text":"<ol> <li>Alias kubectl</li> </ol> <pre><code>alias k=kubectl\n</code></pre> <ol> <li>Explore the cluster</li> </ol> <pre><code>k get nodes\nk get pods\nk get pods -o wide\n</code></pre> <ol> <li>Get cluster information</li> </ol> <pre><code>k cluster-info\nk get componentstatuses\nk get events\nk api-resources\nk api-versions\n</code></pre>"},{"location":"kubernetes/config/","title":"Accessing Kubernetes Clusters","text":""},{"location":"kubernetes/config/#context-and-kubeconfig","title":"Context and kubeconfig","text":"<p><code>kubectl</code> allows a user to interact with and manage multiple Kubernetes clusters. To do this, it requires what is known as a context. A context consists of a combination of <code>cluster</code>, <code>namespace</code> and <code>user</code>. * cluster - A friendly name, server address, and certificate for the Kubernetes cluster. * namespace (optional) - The logical cluster or environment to use. If none is provided, it will use the default <code>default</code> namespace. * user - The credentials used to connect to the cluster. This can be a combination of client certificate and key, username/password, or token.</p> <p>These contexts are stored in a local yaml based config file referred to as the <code>kubeconfig</code>. For *nix based systems, the <code>kubeconfig</code> is stored in <code>$HOME/.kube/config</code> for Windows, it can be found in <code>%USERPROFILE%/.kube/config</code></p> <p>This config is viewable without having to view the file directly.</p> <p>Command <pre><code>$ kubectl config view\n</code></pre></p> <p>Example <pre><code>\u276f kubectl config view\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: DATA+OMITTED\n    server: https://127.0.0.1:46347\n  name: kind-kind\ncontexts:\n- context:\n    cluster: kind-kind\n    user: kind-kind\n  name: kind-kind\ncurrent-context: kind-kind\nkind: Config\npreferences: {}\nusers:\n- name: kind-kind\n  user:\n    client-certificate-data: REDACTED\n    client-key-data: REDACTED\n</code></pre></p>"},{"location":"kubernetes/config/#kubectl-config","title":"<code>kubectl config</code>","text":"<p>Managing all aspects of contexts is done via the <code>kubectl config</code> command. Some examples include: * See the active context with <code>kubectl config current-context</code>. * Get a list of available contexts with <code>kubectl config get-contexts</code>. * Switch to using another context with the <code>kubectl config use-context &lt;context-name&gt;</code> command. * Add a new context with <code>kubectl config set-context &lt;context name&gt; --cluster=&lt;cluster name&gt; --user=&lt;user&gt; --namespace=&lt;namespace&gt;</code>.</p> <p>There can be quite a few specifics involved when adding a context, for the available options, please see the Configuring Multiple Clusters Kubernetes documentation.</p>"},{"location":"kubernetes/config/#exercise-using-contexts","title":"Exercise: Using Contexts","text":"<p>Objective: Create a new context called <code>kind-dev</code> and switch to it.</p> <ol> <li> <p>View the current contexts. <pre><code>$ kubectl config get-contexts\n</code></pre></p> </li> <li> <p>Create a new context called <code>kind-dev</code> within the <code>kind-kind</code> cluster with the <code>dev</code> namespace, as the <code>kind-kind</code> user. <pre><code>$ kubectl config set-context kind-dev --cluster=kind-kind --user=kind-kind --namespace=dev\n</code></pre></p> </li> <li> <p>View the newly added context. <pre><code>kubectl config get-contexts\n</code></pre></p> </li> <li> <p>Switch to the <code>kind-dev</code> context using <code>use-context</code>. <pre><code>$ kubectl config use-context kind-dev\n</code></pre></p> </li> <li> <p>View the current active context. <pre><code>$ kubectl config current-context\n</code></pre></p> </li> </ol> <p>Summary: Understanding and being able to switch between contexts is a base fundamental skill required by every Kubernetes user. As more clusters and namespaces are added, this can become unwieldy. Installing a helper application such as kubectx can be quite helpful. Kubectx allows a user to quickly switch between contexts and namespaces without having to use the full <code>kubectl config use-context</code> command.</p>"},{"location":"kubernetes/deploy-app/","title":"Deploy an application to Kubernetes","text":""},{"location":"kubernetes/deploy-app/#outcomes","title":"Outcomes","text":"<p>You should be able to:</p> <ul> <li>Deploy a simple application</li> </ul>"},{"location":"kubernetes/deploy-app/#outline","title":"Outline","text":"<ol> <li>Deploy a simple application</li> </ol> <pre><code>kubectl create deployment nginx --image=nginx\nkubectl get deployments\nkubectl describe deployment nginx\nkubectl get deployment nginx -o yaml\n</code></pre> <ol> <li>Save the deployment to a file and re-apply it</li> </ol> <pre><code>kubectl get deployment nginx -o yaml &gt; nginx.yaml\nkubectl delete deployment nginx\nkubectl apply -f nginx.yaml\nkubectl get pods\n</code></pre> <ol> <li>Duplicate the deployment</li> </ol> <pre><code>cp nginx.yaml nginx2.yaml\nsed -i 's/nginx/nginx2/g' nginx2.yaml\nkubectl apply -f nginx2.yaml\nkubectl delete deployment\nkubectl get pods\n</code></pre> <ol> <li>Inspect the deployment</li> </ol> <pre><code>kubectl get deployment nginx -o json\nkubectl get deployment nginx -o yaml\nkubectl get deployment nginx -o wide\nkubectl get deployment nginx -o jsonpath='{.spec}'\n</code></pre>"},{"location":"kubernetes/expose/","title":"Expose an application inside the cluster","text":""},{"location":"kubernetes/expose/#outcomes","title":"Outcomes","text":"<p>You should be able to:  - Expose an application inside the cluster  - Reach the application from outside the cluster (using kubectl port-forward)  - Understand the different types of services</p>"},{"location":"kubernetes/expose/#outline","title":"Outline","text":"<ol> <li>Expose an application inside the cluster</li> </ol> <pre><code>kubectl expose deployment nginx --port=80 --target-port=80\nkubectl get services\nkubectl describe service nginx\n</code></pre> <ol> <li>Reach the application from another pod:</li> </ol> <pre><code>kubectl run tmp-shell  --rm -i --tty --image nicolaka/netshoot -- /bin/bash\n&gt; curl nginx\n</code></pre> <ol> <li>Reach the application from outside the cluster (using kubectl port-forward)</li> </ol> <pre><code>kubectl port-forward service/nginx 8080:80 &amp;\ncurl localhost:8080\n\n#to kill the process\nfg\n#CTRL+C\n</code></pre>"},{"location":"kubernetes/labels/","title":"Labels and Selectors","text":""},{"location":"kubernetes/labels/#labels-and-selectors_1","title":"Labels and Selectors","text":"<p>Labels are key-value pairs that are used to identify, describe and group together related sets of objects or resources.</p> <p>Selectors use labels to filter or select objects, and are used throughout Kubernetes.</p>"},{"location":"kubernetes/labels/#exercise-using-labels-and-selectors","title":"Exercise: Using Labels and Selectors","text":"<p>Objective: Explore the methods of labeling objects in addition to filtering them with both equality and set-based selectors.</p> <p>1) Label the Pod <code>pod-example</code> with <code>app=nginx</code> and <code>environment=dev</code> via <code>kubectl</code>.</p> <pre><code>$ kubectl label pod pod-example app=nginx environment=dev\n</code></pre> <p>2) View the labels with <code>kubectl</code> by passing the <code>--show-labels</code> flag <pre><code>$ kubectl get pods --show-labels\n</code></pre></p> <p>3) Update the multi-container example manifest created previously with the labels <code>app=nginx</code> and <code>environment=prod</code> then apply it via <code>kubectl</code>.</p> <p>manifests/pod-multi-container-example.yaml <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: multi-container-example\n  labels:\n    app: nginx\n    environment: prod\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable-alpine\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: html\n      mountPath: /usr/share/nginx/html\n  - name: content\n    image: alpine:latest\n    volumeMounts:\n    - name: html\n      mountPath: /html\n    command: [\"/bin/sh\", \"-c\"]\n    args:\n      - while true; do\n          date &gt;&gt; /html/index.html;\n          sleep 5;\n        done\n  volumes:\n  - name: html\n    emptyDir: {}\n</code></pre></p> <p>Command <pre><code>$ kubectl apply -f manifests/pod-multi-container-example.yaml\n</code></pre></p> <p>4) View the added labels with <code>kubectl</code> by passing the <code>--show-labels</code> flag once again. <pre><code>$ kubectl get pods --show-labels\n</code></pre></p> <p>5) With the objects now labeled, use an equality based selector  targeting the <code>prod</code> environment.</p> <pre><code>$ kubectl get pods --selector environment=prod\n</code></pre> <p>6) Do the same targeting the <code>nginx</code> app with the short version of the selector flag (<code>-l</code>). <pre><code>$ kubectl get pods -l app=nginx\n</code></pre></p> <p>7) Use a set-based selector to view all pods where the <code>app</code> label is <code>nginx</code> and filter out any that are in the <code>prod</code> environment.</p> <pre><code>$ kubectl get pods -l 'app in (nginx), environment notin (prod)'\n</code></pre> <p>Summary: Kubernetes makes heavy use of labels and selectors in near every aspect of it. The usage of selectors may seem limited from the cli, but the concept can be extended to when it is used with higher level resources and objects.</p>"},{"location":"kubernetes/logs/","title":"Access pods' logs","text":""},{"location":"kubernetes/logs/#outcomes","title":"Outcomes","text":"<p>You should be able to: - Access pods' logs - Understand how to debug a pod</p>"},{"location":"kubernetes/logs/#outline","title":"Outline","text":"<ol> <li>Access pods' logs</li> </ol> <pre><code>kubectl logs nginx-&lt;TAB&gt;\nkubectl logs nginx-&lt;TAB&gt; -f\nkubectl logs nginx-&lt;TAB&gt; --previous\n</code></pre> <ol> <li>Access pods' logs using kubectl exec</li> </ol> <pre><code>kubectl exec -it nginx-&lt;TAB&gt; -- /bin/bash\n&gt; ls\n&gt; cat /var/log/nginx/access.log\n&gt; exit\n</code></pre>"},{"location":"kubernetes/ns/","title":"Namespaces","text":""},{"location":"kubernetes/ns/#create-a-namespace","title":"Create a namespace","text":"<pre><code>kubectl create namespace my-namespace\n</code></pre>"},{"location":"kubernetes/ns/#list-namespaces","title":"List namespaces","text":"<pre><code>kubectl get namespaces\n</code></pre>"},{"location":"kubernetes/ns/#switch-namespace","title":"Switch namespace","text":"<pre><code>kubectl config set-context --current --namespace=my-namespace\n</code></pre>"},{"location":"kubernetes/ns/#delete-a-namespace","title":"Delete a namespace","text":"<pre><code>kubectl delete namespace my-namespace\n</code></pre>"},{"location":"kubernetes/scale/","title":"Scale an application on Kubernetes","text":""},{"location":"kubernetes/scale/#outcomes","title":"Outcomes","text":"<p>You should be able to: - Scale an application on Kubernetes</p>"},{"location":"kubernetes/scale/#outline","title":"Outline","text":"<ol> <li>Scale an application on Kubernetes</li> </ol> <pre><code>kubectl scale deployment nginx --replicas=3\nkubectl get pods\n</code></pre> <ol> <li>Scale an application on Kubernetes using kubectl edit</li> </ol> <pre><code>kubectl edit deployment nginx\n</code></pre> <ol> <li>Scale an application on Kubernetes using kubectl patch</li> </ol> <pre><code>kubectl patch deployment nginx -p '{\"spec\":{\"replicas\":2}}'\n</code></pre> <ol> <li>Test the resilience of the application</li> </ol> <pre><code>kubectl delete pod nginx-&lt;TAB&gt;\nkubectl get pods\n</code></pre> <ol> <li>Scale an application on Kubernetes using kubectl autoscale</li> </ol> <pre><code>kubectl autoscale deployment nginx --min=2 --max=5 --cpu-percent=80\nkubectl get hpa\nkubectl get pods\n</code></pre>"},{"location":"kubernetes/services/","title":"Exposing applications using Services","text":""},{"location":"kubernetes/services/#outcomes","title":"Outcomes","text":"<p>In this lab you will learn how to:</p> <ul> <li>Expose applications using Services</li> <li>Use the <code>kubectl port-forward</code> command to access applications running in a Kubernetes cluster</li> <li>Differentiate between the different types of Services</li> </ul>"},{"location":"kubernetes/services/#what-is-a-kubernetes-service","title":"What is a Kubernetes Service?","text":"<p>In Kubernetes, a Service is an abstraction that defines a logical set of Pods and a policy by which to access them. Services provide a way to expose your applications to other services, external traffic, or both. Kubernetes Services can automatically discover and load-balance traffic across multiple pods, making it easy to scale applications and provide reliable access.</p>"},{"location":"kubernetes/services/#key-benefits-of-kubernetes-services","title":"Key Benefits of Kubernetes Services:","text":"<ul> <li>Load balancing: Distribute traffic across a set of Pods to ensure reliability and scalability.</li> <li>Service discovery: Enable communication between services using DNS.</li> <li>Abstracting Pod IPs: Services provide stable endpoints, even if the underlying Pods change.</li> <li>External access: Expose applications outside the cluster to make them accessible via the internet or local network.</li> </ul>"},{"location":"kubernetes/services/#types-of-kubernetes-services","title":"Types of Kubernetes Services","text":"<p>There are four main types of Kubernetes Services:</p> <ol> <li>ClusterIP (default): Exposes the service within the cluster, making it accessible only internally.</li> <li>NodePort: Exposes the service on a specific port on each Node in the cluster, allowing external access.</li> <li>LoadBalancer: Exposes the service externally using a cloud provider\u2019s load balancer.</li> <li>ExternalName: Maps the service to an external DNS name.</li> </ol>"},{"location":"kubernetes/services/#1-clusterip","title":"1. ClusterIP","text":"<p>ClusterIP is the default type of Service. It creates an internal IP address for the service, which is only accessible from within the cluster.</p>"},{"location":"kubernetes/services/#example-clusterip-service","title":"Example: ClusterIP Service","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-clusterip-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n  type: ClusterIP\n</code></pre> <p>In this example: - The service listens on port <code>80</code> and forwards traffic to port <code>8080</code> of the Pods with the label <code>app: my-app</code>. - The service is only accessible within the cluster (no external access).</p>"},{"location":"kubernetes/services/#2-nodeport","title":"2. NodePort","text":"<p>NodePort exposes the service on a specific port on each Node, making it accessible from outside the cluster.</p>"},{"location":"kubernetes/services/#example-nodeport-service","title":"Example: NodePort Service","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-nodeport-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n      nodePort: 30007\n  type: NodePort\n</code></pre> <p>In this example: - The service is exposed on port <code>80</code>, forwarding traffic to port <code>8080</code> of the Pods. - External traffic can access the service via port <code>30007</code> on any Kubernetes Node.</p>"},{"location":"kubernetes/services/#3-loadbalancer","title":"3. LoadBalancer","text":"<p>LoadBalancer creates an external load balancer, typically using cloud provider integrations. This service type allows external access to applications through a load balancer, such as those provided by AWS, GCP, or Azure.</p>"},{"location":"kubernetes/services/#example-loadbalancer-service","title":"Example: LoadBalancer Service","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-loadbalancer-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n  type: LoadBalancer\n</code></pre> <p>In this example: - The service listens on port <code>80</code> and forwards traffic to port <code>8080</code> of the Pods. - A cloud-based load balancer is provisioned to handle external traffic.</p>"},{"location":"kubernetes/services/#4-externalname","title":"4. ExternalName","text":"<p>ExternalName is a special type of Service that maps to a DNS name, allowing Kubernetes services to access external resources.</p>"},{"location":"kubernetes/services/#example-externalname-service","title":"Example: ExternalName Service","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-externalname-service\nspec:\n  type: ExternalName\n  externalName: external.example.com\n</code></pre> <p>In this example: - The service maps to the external DNS <code>external.example.com</code>, allowing Pods inside the cluster to access the external resource by calling <code>my-externalname-service</code>.</p>"},{"location":"kubernetes/services/#using-kubectl-port-forward-to-access-applications","title":"Using <code>kubectl port-forward</code> to Access Applications","text":"<p>In some cases, you may want to access your application directly from your local machine without exposing it externally using a Service. The <code>kubectl port-forward</code> command allows you to forward a local port to a port on a Pod, providing access to applications running inside your Kubernetes cluster.</p>"},{"location":"kubernetes/services/#example-port-forwarding-to-a-pod","title":"Example: Port Forwarding to a Pod","text":"<p>You can use the following command to forward a local port to a Pod in your cluster:</p> <pre><code>kubectl port-forward pod/my-app-pod 8080:80\n</code></pre> <p>This command forwards local port <code>8080</code> to port <code>80</code> on the Pod <code>my-app-pod</code>. You can then access the application by visiting <code>http://localhost:8080</code> in your browser.</p> <p>You can also forward a Service port instead of a Pod port:</p> <pre><code>kubectl port-forward svc/my-clusterip-service 8080:80\n</code></pre> <p>This will forward the local port <code>8080</code> to port <code>80</code> on the Service <code>my-clusterip-service</code>.</p>"},{"location":"kubernetes/services/#best-practices-for-using-services","title":"Best Practices for Using Services","text":"<ol> <li>Choose the appropriate service type: Use <code>ClusterIP</code> for internal communication, <code>NodePort</code> for limited external access, and <code>LoadBalancer</code> for fully managed external access.</li> <li>Use labels and selectors effectively: Ensure that your Service's selectors match the correct Pods to route traffic correctly.</li> <li>Limit NodePort usage: Since NodePort opens specific ports on each Node, limit its use to cases where you need direct access from outside the cluster.</li> <li>Use DNS for service discovery: Kubernetes automatically creates DNS entries for Services, so you can access Services by their name (e.g., <code>my-service.default.svc.cluster.local</code>).</li> </ol>"},{"location":"kubernetes/services/#summary","title":"Summary","text":"<p>In Kubernetes, Services are essential for exposing applications, whether internally or externally. By understanding the different types of Services\u2014ClusterIP, NodePort, LoadBalancer, and ExternalName\u2014you can choose the best method to expose your applications based on your needs. Additionally, <code>kubectl port-forward</code> provides a quick way to access applications directly without exposing them externally. Implementing best practices around Services ensures that your applications are scalable, reliable, and secure.</p>"},{"location":"kubernetes/advanced/","title":"Advanced Topics","text":"<p>Advanced Kubernetes topics</p>"},{"location":"kubernetes/advanced/access-kube-api/","title":"Tutorial: How to access the Kubernetes API from a pod","text":"<p>Kubernetes by default exposes its own API server to workloads within the cluster via a special service in the <code>default</code> namespace.</p>"},{"location":"kubernetes/advanced/access-kube-api/#access-the-api","title":"Access the API","text":"<p>The service <code>kubernetes</code> in the <code>default</code> namespace has no selector, and it's created by the API server itself at startup; it has an hardcoded endpoint pointing to the internal IP of the API server.</p> <p>We can run an unprivileged pod in the default namespace and access the token:</p> <pre><code>kubectl run -it --tty --rm debug --image=curlimages/curl --restart=Never -- sh\n</code></pre> <p>Now curl the API endpoint; notice the use of the variable substitution (Kubernetes automatically populates a number of env vars into each pod):</p> <pre><code>curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt \\\n     -H \"Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" \\\n     https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_SERVICE_PORT\n</code></pre> <p>You'll get a request denied, but you have accessed the API nonetheless:</p> <pre><code>{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {},\n  \"status\": \"Failure\",\n  \"message\": \"forbidden: User \\\"system:serviceaccount:default:default\\\" cannot get path \\\"/\\\"\",\n  \"reason\": \"Forbidden\",\n  \"details\": {},\n  \"code\": 403\n}\n</code></pre> <p>You'll need to assign a (cluster)role and a (cluster)rolebinding to the Service Account to make it work.</p>"},{"location":"kubernetes/advanced/crds/","title":"Custom Resource Definitions","text":""},{"location":"kubernetes/advanced/crds/#check-what-crds-are","title":"Check what CRDs are","text":"<pre><code>$ kubectl get crd\n\n$ kubectl get crd &lt;crd&gt; -o yaml\n</code></pre> <p>Get instances of a specific crd</p> <pre><code>$ kubectl get &lt;crd_name&gt; -A\n</code></pre>"},{"location":"kubernetes/advanced/crds/#node-debugging","title":"Node debugging","text":"<pre><code>$ kubectl debug node/node_name --it --image bash\n</code></pre> <p>Or use the <code>node-admin</code> krew plugin. Another cool trick:</p> <pre><code>kubectl run h0nk --rm -it \\\n --image alpine --privileged \\\n --overrides '{\"spec\":{\"hostPID\": true}}'\\\n --command nsenter \u2013 \\\n --mount=/proc/1/ns/mnt\n</code></pre> <p>From Ian Coldwater tweet </p>"},{"location":"kubernetes/advanced/debugging/","title":"Debugging","text":""},{"location":"kubernetes/advanced/debugging/#pod-debugging","title":"Pod debugging","text":"<pre><code>$ kubectl exec -it broken-pod -- sh\n</code></pre> <p>Make sure the shell exists! And containers need to be running, if they keep crashing this is pretty useless. You can attach a debug pod:</p> <pre><code>$ kubectl debug -it --image bash restarting-pod #(or nicolaka/netshoot) \n</code></pre>"},{"location":"kubernetes/advanced/init-containers/","title":"Init containers","text":"<p>Init containers are containers that run before the main container starts. They're useful for many applications, where some preparation steps (database initialization, cache warmup etc) are needed.</p>"},{"location":"kubernetes/advanced/init-containers/#outcomes","title":"Outcomes","text":"<p>In this lab, you will learn how to: - Create a pod with an init container - View the logs of an init container - View the status of an init container - Delete a pod with an init container</p>"},{"location":"kubernetes/advanced/pod-deletion/","title":"What happens when a pod is deleted","text":"<p>What happens behind the scene when a pod gets deleted?</p> <p>When we do:  <pre><code>$ kubectl delete pod nginx\n\npod \"nginx\" deleted\n</code></pre></p> <p>What actually happens behind the scenes?</p> <p>Before continuing, let's remind ourselves of the linux signals that are available to us from the container perspective:</p> <p>SIGTERM: Requests a graceful shutdown, allowing the program to finish tasks and clean up. In Kubernetes, pods get time to exit cleanly.</p> <p>SIGKILL: Forces an immediate stop, with no cleanup. If a pod doesn't shut down in time after SIGTERM, Kubernetes sends SIGKILL to terminate it immediately.</p> <p></p> <ol> <li>kubectl delete pod: Triggers the API Server to update ETCD with deletionTimestamp and deletionGracePeriodSeconds, marking the pod as Terminating.</li> <li>API Server \u2192 Kubelet: Notifies the Kubelet of the pod\u2019s termination.</li> <li>Endpoint Controller: Removes the pod from active service endpoints, stopping any traffic from reaching the pod.</li> <li>PreStop Hook (if configured): Before sending SIGTERM, the Kubelet runs the PreStop Hook. This allows the pod to perform custom tasks (e.g., closing connections) during shutdown.</li> <li>SIGTERM: Kubelet sends SIGTERM, initiating a graceful shutdown. The pod is given the deletionGracePeriodSeconds (default 30s) to cleanly exit.</li> <li>Graceful Shutdown: During the grace period, the pod handles any ongoing tasks, such as completing requests or saving data, before it fully stops.</li> <li>SIGKILL: If the pod doesn't terminate within the grace period, SIGKILL is sent, forcing immediate shutdown.</li> <li>Pod Deleted: The API Server updates ETCD, marking the pod as deleted. Components like Kube-Proxy, Ingress, and others remove all references to the pod.</li> </ol>"},{"location":"kubernetes/advanced/resource-quotas/","title":"Resource Quotas","text":""},{"location":"kubernetes/advanced/resource-quotas/#outcomes","title":"Outcomes","text":"<p>In this lab, you will learn how to: - Create a Resource Quota - View Resource Quota usage - View Resource Quota status - Delete a Resource Quota</p>"},{"location":"kubernetes/advanced/resource-quotas/#outline","title":"Outline","text":"<p>Create a Resource Quota:</p> <pre><code>kubectl create quota my-quota --hard=cpu=1,memory=1G,pods=2,services=2,replicationcontrollers=2,resourcequotas=1\n</code></pre>"},{"location":"kubernetes/advanced/rolling-upgrades/","title":"Rolling upgrades","text":""},{"location":"kubernetes/cli/","title":"CLIs","text":"<p>This is a collection of instructions on how to install various Kubernetes command line tools on various platforms.</p>"},{"location":"kubernetes/cli/completion/","title":"Auto-completion","text":"<p>Enable auto-completion for kubectl:</p> <pre><code>source &lt;(kubectl completion bash)\necho \"source &lt;(kubectl completion bash)\" &gt;&gt; $HOME/.bashrc\n</code></pre>"},{"location":"kubernetes/cli/docker-desktop/","title":"Docker Desktop","text":"<p>Docker Desktop includes a single-node Kubernetes cluster that runs on your Mac or Windows machine. It also includes a standalone Kubernetes server and client, as well as Docker CLI integration. It is the preferred way to get started with Kubernetes on your local machine, as it makes it very easy to expose services running in Kubernetes to your local machine via localhost.</p>"},{"location":"kubernetes/cli/krew/","title":"Krew","text":"<p>Krew is a plugin manager for <code>kubectl</code>. Install it with:</p> <p><pre><code>(\n  set -x; cd \"$(mktemp -d)\" &amp;&amp;\n  OS=\"$(uname | tr '[:upper:]' '[:lower:]')\" &amp;&amp;\n  ARCH=\"$(uname -m | sed -e 's/x86_64/amd64/' -e 's/\\(arm\\)\\(64\\)\\?.*/\\1\\2/' -e 's/aarch64$/arm64/')\" &amp;&amp;\n  KREW=\"krew-${OS}_${ARCH}\" &amp;&amp;\n  curl -fsSLO \"https://github.com/kubernetes-sigs/krew/releases/latest/download/${KREW}.tar.gz\" &amp;&amp;\n  tar zxvf \"${KREW}.tar.gz\" &amp;&amp;\n  ./\"${KREW}\" install krew\n)\n</code></pre> Add the krew plugin path to your shell:</p> <pre><code>echo \"export PATH=\"${KREW_ROOT:-$HOME/.krew}/bin:$PATH\"\" &gt;&gt; ~/.bashrc #or ~/.zshrc\n</code></pre>"},{"location":"kubernetes/cli/kubectl/","title":"Kubectl","text":"<p>Install Kubectl:</p> <p>Windows:</p> <pre><code>choco install kubernetes-cli\n</code></pre> <p>Linux:</p> <pre><code>curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.21.0/bin/linux/amd64/kubectl\nchmod +x ./kubectl\nsudo mv ./kubectl /usr/local/bin/kubectl\n</code></pre> <p>MacOS:</p> <pre><code>brew install kubectl\n</code></pre> <p>Get help:</p> <p>```bash kubectl --help kubectl help kubectl help create</p>"},{"location":"kubernetes/cli/kubectx/","title":"Kubectx","text":"<p>Kubextx and kubens are useful commands to switch between contexts and namespaces.</p> <p>They're best deployed as krew plugins:</p> <pre><code>kubectl krew install ctx\nkubectl krew install ns\n</code></pre>"},{"location":"kubernetes/cli/stern/","title":"Install Stern to access pod logs","text":"<p>Stern is a tool that allows you to tail multiple pods on Kubernetes and multiple containers within the pod. Each result is color coded for quicker debugging.</p> <ol> <li> <p>Install Stern</p> <pre><code>curl -L -o /usr/local/bin/stern\nchmod +x /usr/local/bin/stern\n</code></pre> </li> <li> <p>Install on MacOS</p> </li> </ol> <pre><code>brew install stern\n</code></pre>"},{"location":"kubernetes/configsecrets/","title":"ConfigMaps and Secrets","text":""},{"location":"kubernetes/configsecrets/#introduction","title":"Introduction","text":"<p>ConfigMaps and Secrets</p>"},{"location":"kubernetes/configsecrets/#configmaps","title":"ConfigMaps","text":"<p>Create a ConfigMap using both literals and files:</p> <pre><code>$ echo m &gt; primary/magenta\n$ echo y &gt; primary/yellow\n$ echo k &gt; primary/black\n$ echo \"known as key\" &gt;&gt; primary/black\n$ echo blue &gt; favorite\n\n$ kubectl create configmap colors \\\n--from-literal=text=black \\\n--from-file=./favorite \\\n--from-file=./primary/\n\nconfigmap/colors created\n\n$ kubectl get configmap colors\n$ kubectl get configmap colors -o yaml\n</code></pre> <p>Create a simple pod that consumes the ConfigMap</p> <pre><code>$|kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: shell-demo\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    env:\n    - name: ilike\n      valueFrom:\n        configMapKeyRef:\n          name: colors\n          key: favorite\nEOF\n</code></pre> <p>Extract the information from the pod:</p> <pre><code>$ kubectl exec shell-demo -- /bin/bash -c 'echo $ilike'\nblue\n</code></pre>"},{"location":"kubernetes/configsecrets/#secrets","title":"Secrets","text":""},{"location":"kubernetes/deploy/","title":"Deploy a Kubernetes cluster","text":"<p>We will describe a number of ways to deploy a cluster locally. One relies on docker and it's Kubernetes-in-Docker (kind), the other can leverage various hypervisors and it's called minikube. A native way to deploy Kubernetes is kubeadm.</p>"},{"location":"kubernetes/deploy/advanced-kind/","title":"Advanced Cluster Deployment with Kind","text":"<p>We will use this script: kind_cilium.sh to deploy a cluster with multiple nodes, support for LoadBalancer type services, and more.</p> <p>Download the script:</p> <pre><code>curl -O kind_cilium.sh https://gist.githubusercontent.com/ams0/4f1063be9e8d5c34fc85a1b4857aed71/raw/2fc80bf98d66e0b265fedd77a27d6c66e3e36627/kind_cilium.sh\n\nchmox +x kind_cilium.sh\n</code></pre> <p>Deploy a cluster with Cilium and MetalLB:</p> <pre><code>./kind_cilium.sh -n kind -t true -c true\n</code></pre> <p>Check that Cilium and MetalLB are deployed</p> <pre><code>kubectl get po -n kube-system -l app.kubernetes.io/name=cilium-agent\nkubectl get po -n metallb-system\n</code></pre>"},{"location":"kubernetes/deploy/deploy-kubeadm/","title":"Kubeadm","text":"<p>Kubeadm is a tool to install and configure the various components of a Kubernetes cluster on one or multiple nodes; it deploys the various components of the Kubernetes control plane and is the preferred tool for installing Kubernetes on bare metal.</p> <p>Install Kubeadm on linux (not avaialble for MacOS or Windows):</p> <pre><code>apt-get update &amp;&amp; apt-get install -y apt-transport-https curl\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\ncat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list\ndeb https://apt.kubernetes.io/ kubernetes-xenial main\nEOF\napt-get update\napt-get install -y kubelet kubeadm kubectl\n</code></pre>"},{"location":"kubernetes/deploy/deploy-minikube/","title":"Deploy a cluster with minikube","text":"<p>Visit minikube website and choose your operating system and architecture; it'll give you a set of commands to execute:</p> <pre><code>curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64\nsudo install minikube-linux-amd64 /usr/local/bin/minikube &amp;&amp; rm minikube-linux-amd64\n</code></pre> <p>After that, create a simple single-node, docker-based cluster:</p> <pre><code>$ minikube start\n</code></pre> <p>Check the cluster is alive</p> <p>```bash $ kubectl cluster-info</p> <p>$ kubectl </p>"},{"location":"kubernetes/networking/","title":"Kubernetes Networking","text":""},{"location":"kubernetes/networking/dns/","title":"Kubernetes service discovery with DNS","text":""},{"location":"kubernetes/networking/dns/#outcomes","title":"Outcomes","text":"<ul> <li>Understand how DNS works in Kubernetes</li> <li>Understand how to use DNS for service discovery</li> </ul>"},{"location":"kubernetes/networking/dns/#outline","title":"Outline","text":""},{"location":"kubernetes/networking/network-policies/","title":"Network Policies","text":""},{"location":"kubernetes/networking/network-policies/#lets-demostrate-the-power-of-network-policies","title":"Let's demostrate the power of Network Policies","text":"<p>Create an <code>nginx</code> deployment and expose its service:</p> <pre><code>kubectl create deploy nginx --replicas 2 --image nginx\nkubectl expose deploy nginx --port 80\n</code></pre> <p>Create a busybox pod:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  containers:\n  - command:\n    - sh\n    - -c\n    - while true; do echo hello; sleep 10;done\n    image: busybox\n    name: busybox\nEOF\n\n\nCreate a policy to block all traffic to nginx pods except for pods with label access=true\n\n```bash\nkubectl apply -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: access-nginx\nspec:\n  podSelector:\n    matchLabels:\n      app: nginx\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          access: \"true\"\nEOF\n</code></pre> <p>Allow the busybox pod to access nginx pods </p> <pre><code>kubectl label po busybox access=true\n</code></pre>"},{"location":"kubernetes/nodes/","title":"Node management","text":""},{"location":"kubernetes/nodes/#outcomes","title":"Outcomes","text":"<ul> <li>Understand the role of a node in a Kubernetes cluster</li> <li>Understand how to add and remove nodes from a cluster</li> <li>Understand how to configure nodes</li> <li>Understand how to manage nodes</li> <li>Cordon and uncordon nodes</li> <li>Drain nodes</li> </ul>"},{"location":"kubernetes/nodes/#outline","title":"Outline","text":"<p>A node is a physical or virtual machine on which pods and other resources that underpin your workload run. Each node is managed by a master node known as a control pane which contains multiple services required to run pods. A cluster typically has multiple nodes.</p> <p>To get the number of nodes on your cluster, run:</p> <pre><code>$ kubectl get nodes\nList Nodes in Kubernetes Cluster\nList Nodes in Kubernetes Cluster\nTo get the pods running on a node, execute:\n</code></pre> <pre><code>$ kubectl get pods -o wide | grep &lt;node_name&gt;\nList Pods Running on Kubernetes Cluster\nList Pods Running on Kubernetes Cluster\nTo mark your node as unschedulable, run.\n\n$ kubectl cordon minikube\n\nnode/minikube cordoned\nTo mark your node as schedulable, run.\n</code></pre> <pre><code>$ kubectl uncordon minikube\n\nnode/minikube uncordoned\nTo display resource usage metrics such as RAM and CPU run:\n$ kubectl top node &lt;node_name&gt;\nTo delete a node or multiple nodes, run the command:\n\n$ kubectl delete node &lt;node_name&gt;\n</code></pre>"},{"location":"kubernetes/storage/","title":"Storage","text":""},{"location":"kubernetes/storage/#create-a-pod-with-an-ephemeral-volume","title":"Create a pod with an ephemeral volume","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: fordpinto\n  namespace: default\nspec:\n  containers:\n    - image: simpleapp\n      name: gastank\n      command:\n        - sleep\n        - \"3600\"\n      volumeMounts:\n        - mountPath: /scratch\n          name: scratch-volume\n  volumes:\n    - name: scratch-volume\n      emptyDir: {}\nEOF\n</code></pre>"},{"location":"kubernetes/storage/#creating-a-persistent-volume-pv","title":"Creating a Persistent Volume (PV)","text":"<pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: my-pv\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n    - ReadWriteOnce\n  hostPath:\n    path: \"/mnt/data\"\nEOF\n</code></pre> <p>Create a PVC</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: my-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 5Gi\nEOF\n</code></pre>"},{"location":"kubernetes/storage/statefulset/","title":"StatefulSets","text":"<p>The StatefulSet controller is tailored to managing Pods that must persist or maintain state. Pod identity including hostname, network, and storage can be considered persistent.</p> <p>They ensure persistence by making use of three things: * The StatefulSet controller enforcing predicable naming, and ordered provisioning/updating/deletion. * A headless service to provide a unique network identity. * A volume template to ensure stable per-instance storage.</p>"},{"location":"kubernetes/storage/statefulset/#exercise-managing-statefulsets","title":"Exercise: Managing StatefulSets","text":"<p>Objective: Create, update, and delete a <code>StatefulSet</code> to gain an understanding of how the StatefulSet lifecycle differs from other workloads with regards to updating, deleting and the provisioning of storage.</p> <p>1) Create StatefulSet <code>sts-example</code> using the yaml block below or the manifest <code>manifests/sts-example.yaml</code>.</p> <p>manifests/sts-example.yaml <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sts-example\nspec:\n  replicas: 3\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app: stateful\n  serviceName: app\n  updateStrategy:\n    type: OnDelete\n  template:\n    metadata:\n      labels:\n        app: stateful\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:stable-alpine\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: www\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      storageClassName: standard\n      resources:\n        requests:\n          storage: 1Gi\n</code></pre></p> <p>Command <pre><code>$ kubectl create -f manifests/sts-example.yaml\n</code></pre></p> <p>2) Immediately watch the Pods being created. <pre><code>$ kubectl get pods --show-labels --watch\n</code></pre> Unlike Deployments or DaemonSets, the Pods of a StatefulSet are created one-by-one, going by their ordinal index. Meaning, <code>sts-example-0</code> will fully be provisioned before <code>sts-example-1</code> starts up. Additionally, take notice of the <code>controller-revision-hash</code> label. This serves the same purpose as the <code>controller-revision-hash</code> label in a DaemonSet or the <code>pod-template-hash</code> in a Deployment. It provides a means of tracking the revision of the Pod Template and enables rollback functionality.</p> <p>3) More information on the StatefulSet can be gleaned about the state of the StatefulSet by describing it. <pre><code>$ kubectl describe statefulset sts-example\n</code></pre> Within the events, notice that it is creating claims for volumes before each Pod is created.</p> <p>4) View the current Persistent Volume Claims. <pre><code>$ kubectl get pvc\n</code></pre> The StatefulSet controller creates a volume for each instance based off the <code>volumeClaimTemplate</code>. It prepends the volume name to the Pod name. e.g. <code>www-sts-example-0</code>.</p> <p>5) Update the StatefulSet's Pod Template and add a few additional labels. <pre><code>$ kubectl apply -f manifests/sts-example.yaml --record\n  &lt; or &gt;\n$ kubectl edit statefulset sts-example --record\n</code></pre></p> <p>6) Return to watching the Pods. <pre><code>$ kubectl get pods --show-labels\n</code></pre> None of the Pods are being updated to the new version of the Pod.</p> <p>7) Delete the <code>sts-example-2</code> Pod. <pre><code>$ kubectl delete pod sts-example-2\n</code></pre></p> <p>8) Immediately get the Pods. <pre><code>$ kubectl get pods --show-labels --watch\n</code></pre> The new <code>sts-example-2</code> Pod should be created with the new additional labels. The <code>OnDelete</code> Update Strategy will not spawn a new iteration of the Pod until the previous one was deleted. This allows for manual gating the update process for the StatefulSet.</p> <p>9) Update the StatefulSet and change the Update Strategy Type to <code>RollingUpdate</code>. <pre><code>$ kubectl apply -f manifests/sts-example.yaml --record\n  &lt; or &gt;\n$ kubectl edit statefulset sts-example --record\n</code></pre></p> <p>10) Immediately watch the Pods once again. <pre><code>$ kubectl get pods --show-labels --watch\n</code></pre> Note that the Pods are sequentially updated in descending order, or largest to smallest based on the Pod's ordinal index. This means that if <code>sts-example-2</code> was not updated already, it would be updated first, then <code>sts-example-1</code> and finally <code>sts-example-0</code>.</p> <p>11) Delete the StatefulSet <code>sts-example</code> <pre><code>$ kubectl delete statefulset sts-example\n</code></pre></p> <p>12) View the Persistent Volume Claims. <pre><code>$ kubectl get pvc\n</code></pre> Created PVCs are NOT garbage collected automatically when a StatefulSet is deleted. They must be reclaimed independently of the StatefulSet itself.</p> <p>13) Recreate the StatefulSet using the same manifest. <pre><code>$ kubectl create -f manifests/sts-example.yaml --record\n</code></pre></p> <p>14) View the Persistent Volume Claims again. <pre><code>$ kubectl get pvc\n</code></pre> Note that new PVCs were NOT provisioned. The StatefulSet controller assumes if the matching name is present, that PVC is intended to be used for the associated Pod.</p> <p>Summary: Like many applications where state must be taken into account, the planning and usage of StatefulSets requires forethought. The consistency brought by standard naming, ordered updates/deletes and templated storage does however make this task easier.</p>"},{"location":"kubernetes/storage/statefulset/#exercise-understanding-statefulset-network-identity","title":"Exercise: Understanding StatefulSet Network Identity","text":"<p>Objective: Create a \"headless service\" or a service without a <code>ClusterIP</code> (<code>ClusterIP=None</code>) for use with the StatefulSet <code>sts-example</code>, then explore how this enables consistent service discovery.</p> <p>1) Create the headless service <code>app</code> using the <code>app=stateful</code> selector from the yaml below or the manifest <code>manifests/service-sts-example.yaml</code>.</p> <p>manifests/service-sts-example.yaml <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: app\nspec:\n  clusterIP: None\n  selector:\n    app: stateful\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n</code></pre></p> <p>Command <pre><code>$ kubectl create -f manifests/service-sts-example.yaml\n</code></pre></p> <p>2) Describe the newly created service <pre><code>$ kubectl describe svc app\n</code></pre> Notice that it does not have a <code>clusterIP</code>, but does have the Pod Endpoints listed. Headless services are unique in this behavior.</p> <p>3) Query the DNS entry for the <code>app</code> service. <pre><code>$ kubectl exec sts-example-0 -- nslookup app.default.svc.cluster.local\n</code></pre> An A record will have been returned for each instance of the StatefulSet. Querying the service directly will do simple DNS round-robin load-balancing.</p> <p>4) Finally, query one of instances directly. <pre><code>$ kubectl exec sts-example-0 -- nslookup sts-example-1.app.default.svc.cluster.local\n</code></pre> This is a unique feature to StatefulSets. This allows for services to directly interact with a specific instance of a Pod. If the Pod is updated and obtains a new IP, the DNS record will immediately point to it enabling consistent service discovery.</p> <p>Summary: StatefulSet service discovery is unique within Kubernetes in that it augments a headless service (A service without a unique <code>ClusterIP</code>) to provide a consistent mapping to the individual Pods. These mappings take the form of an A record in format of: <code>&lt;StatefulSet Name&gt;-&lt;ordinal&gt;.&lt;service name&gt;.&lt;namespace&gt;.svc.cluster.local</code> and can be used consistently throughout other Workloads.</p> <p>Clean Up Command <pre><code>kubectl delete svc app\nkubectl delete statefulset sts-example\nkubectl delete pvc www-sts-example-0 www-sts-example-1 www-sts-example-2\n</code></pre></p>"},{"location":"kubernetes/storage/volumes/","title":"Volumes","text":"<p>Make sure you have a storage class in your cluster! You can install the local-storage-provisioner:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.30/deploy/local-path-storage.yaml\n</code></pre> <p>Create a PV:</p> <pre><code>cat &lt;&lt;EOF | k apply -f -\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv-volume\nspec:\n  persistentVolumeReclaimPolicy: Delete\n  storageClassName: \"local-path\"\n  claimRef:\n    name: pv-claim\n    namespace: default\n  hostPath:\n    path: \"/mnt/data\"\n  capacity:\n    storage: 1Gi\n  accessModes:\n    - ReadWriteOnce\nEOF\n</code></pre> <p>Create a PVC:</p> <pre><code>cat &lt;&lt;EOF | k apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pv-claim\n  namespace: default\nspec:\n  storageClassName: \"local-path\"\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\nEOF\n</code></pre> <p>Create the pod:</p> <pre><code>cat &lt;&lt;EOF | k apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pv-pod\nspec:\n  containers:\n    - name: pv-container\n      image: nginx\n      volumeMounts:\n        - mountPath: \"/usr/share/nginx/html\"\n          name: pv-storage\n  volumes:\n    - name: pv-storage\n      persistentVolumeClaim:\n        claimName: pv-claim\nEOF\n</code></pre>"},{"location":"kubernetes/troubleshooting/","title":"Kubernetes Troubleshooting","text":""},{"location":"kubernetes/troubleshooting/#overview","title":"Overview","text":"<p>This section will guide you thru common troubleshooting patterns.</p>"},{"location":"kubernetes/troubleshooting/#cluster-events","title":"Cluster Events","text":"<p>One of the first step for troubleshooting should be to look at the event logs:</p> <pre><code>kubectl get events --all-namespaces\n</code></pre> <p>It's possible also to limit the output to a single namespace or resource. Logs for a resource are also available in the output of the <code>kubectl describe &lt;resource&gt;</code> command.</p>"},{"location":"kubernetes/troubleshooting/#common-scenarios","title":"Common scenarios","text":"<p>Some classic errors:</p>"},{"location":"kubernetes/workloads/","title":"Kubernetes Workloads","text":""},{"location":"kubernetes/workloads/#overview","title":"Overview","text":"<p>Kubernetes workloads are the objects that run your containerized applications. There are several different types of workloads, each with their own use cases and configuration options. This lesson will cover the following workload types:</p> <ul> <li>Pods</li> <li>Deployments</li> <li>StatefulSets</li> <li>DaemonSets</li> <li>Jobs</li> <li>CronJobs</li> <li>ReplicaSets</li> <li>Horizontal Pod Autoscalers</li> <li>PodDisruptionBudgets</li> </ul>"},{"location":"kubernetes/workloads/#objectives","title":"Objectives","text":"<ul> <li>Understand the different types of Kubernetes workloads</li> <li>Understand the use cases for each workload type</li> <li>Understand the configuration options for each workload type</li> </ul>"},{"location":"kubernetes/workloads/#outline","title":"Outline","text":"<ol> <li>ReplicaSets</li> </ol> <p>ReplicaSets are the primary method of managing Pod replicas and their lifecycle. This includes their scheduling, scaling, and deletion.</p> <p>Their job is simple, always ensure the desired number of replicas that match the selector are running.</p> <p>manifests/replicaset.yaml <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: example-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n      env: prod\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: prod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:stable-alpine\n        ports:\n        - containerPort: 80\n</code></pre></p> <p>Command <pre><code>$ kubectl create -f manifests/rs-example.yaml\n</code></pre></p> <p>Watch the ReplicaSet's Pods come up: <pre><code>$ kubectl get pods -l app=nginx,env=prod --watch\n</code></pre></p> <p>Scale the ReplicaSet: <pre><code>$ kubectl scale replicaset example-rs --replicas=5\n</code></pre></p> <p>Create an independent Pod manually with the same labels as the one targeted by rs-example from the manifest manifests/pod-rs-example.yaml</p> <p>manifests/pod-rs-example.yaml <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-example\n  labels:\n    app: nginx\n    env: prod\nspec:\n  containers:\n  - name: nginx\n    image: nginx:stable-alpine\n    ports:\n    - containerPort: 80\n</code></pre></p> <p>Command <pre><code>kubectl create -f manifests/pod-rs-example.yaml\n</code></pre></p>"},{"location":"kubernetes/workloads/#deployments","title":"Deployments","text":"<p>Deployments are a declarative method of managing Pods via ReplicaSets. They provide rollback functionality in addition to more granular update control mechanisms.</p>"},{"location":"kubernetes/workloads/#exercise-using-deployments","title":"Exercise: Using Deployments","text":"<p>Objective: Create, update and scale a Deployment as well as explore the relationship of Deployment, ReplicaSet and Pod.</p> <p>1) Create a Deployment <code>deploy-example</code>. Configure it using the example yaml block below or use the manifest  <code>manifests/deploy-example.yaml</code>. Additionally pass the <code>--record</code> flag to <code>kubectl</code> when you create the Deployment.  The <code>--record</code> flag saves the command as an annotation, and it can be thought of similar to a git commit message.</p> <p>manifests/deployment-example.yaml <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: deploy-example\nspec:\n  replicas: 3\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app: nginx\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:stable-alpine\n        ports:\n        - containerPort: 80\n</code></pre></p> <p>Command <pre><code>$ kubectl create -f manifests/deploy-example.yaml --record\n</code></pre></p> <p>2) Check the status of the Deployment. <pre><code>$ kubectl get deployments\n</code></pre></p> <p>3) Once the Deployment is ready, view the current ReplicaSets and be sure to show the labels. <pre><code>$ kubectl get rs --show-labels\n</code></pre> Note the name and <code>pod-template-hash</code> label of the newly created ReplicaSet. The created ReplicaSet's name will include the <code>pod-template-hash</code>.</p> <p>4) Describe the generated ReplicaSet. <pre><code>$ kubectl describe rs deploy-example-&lt;pod-template-hash&gt;\n</code></pre> Look at both the <code>Labels</code> and the <code>Selectors</code> fields. The <code>pod-template-hash</code> value has automatically been added to both the Labels and Selector of the ReplicaSet. Then take note of the <code>Controlled By</code> field. This will reference the direct parent object, and in this case the original <code>deploy-example</code> Deployment.</p> <p>5) Now, get the Pods and pass the <code>--show-labels</code> flag. <pre><code>$ kubectl get pods --show-labels\n</code></pre> Just as with the ReplicaSet, the Pods name are labels include the <code>pod-template-hash</code>.</p> <p>6) Describe one of the Pods. <pre><code>$ kubectl describe pod deploy-example-&lt;pod-template-hash-&lt;random&gt;\n</code></pre> Look at the <code>Controlled By</code> field. It will contain a reference to the parent ReplicaSet, but not the parent Deployment.</p> <p>Now that the relationship from Deployment to ReplicaSet to Pod is understood. It is time to update the <code>deploy-example</code> and see an update in action.</p> <p>7) Update the <code>deploy-example</code> manifest and add a few additional labels to the Pod template. Once done, apply the change with the <code>--record</code> flag. <pre><code>$ kubectl apply -f manifests/deploy-example.yaml --record\n  &lt; or &gt;\n$ kubectl edit deploy deploy-example --record\n</code></pre> Tip: <code>deploy</code> can be substituted for <code>deployment</code> when using <code>kubectl</code>.</p> <p>8) Immediately watch the Pods. <pre><code>$ kubectl get pods --show-labels --watch\n</code></pre> The old version of the Pods will be phased out one at a time and instances of the new version will take its place. The way in which this is controlled is through the <code>strategy</code> stanza. For specific documentation this feature, see the Deployment Strategy Documentation.</p> <p>9) Now view the ReplicaSets. <pre><code>$ kubectl get rs --show-labels\n</code></pre> There will now be two ReplicaSets, with the previous version of the Deployment being scaled down to 0.</p> <p>10) Now, scale the Deployment up as you would a ReplicaSet, and set the <code>replicas=5</code>. <pre><code>$ kubectl scale deploy deploy-example --replicas=5\n</code></pre></p> <p>11) List the ReplicaSets. <pre><code>$ kubectl get rs --show-labels\n</code></pre> Note that there is NO new ReplicaSet generated. Scaling actions do NOT trigger a change in the Pod Template.</p> <p>12) Just as before, describe the Deployment, ReplicaSet and one of the Pods. Note the <code>Events</code> and <code>Controlled By</code> fields. It should present a clear picture of relationship between objects during an update of a Deployment. <pre><code>$ kubectl describe deploy deploy-example\n$ kubectl describe rs deploy-example-&lt;pod-template-hash&gt;\n$ kubectl describe pod deploy-example-&lt;pod-template-hash-&lt;random&gt;\n</code></pre></p> <p>Summary: Deployments are the main method of managing applications deployed within Kubernetes. They create and supervise targeted ReplicaSets by generating a unique hash called the <code>pod-template-hash</code> and attaching it to child objects as a Label along with automatically including it in their Selector. This method of managing rollouts along with being able to define the methods and tolerances in the update strategy permits for a safe and seamless way of updating an application in place.</p>"},{"location":"kubernetes/workloads/#exercise-rolling-back-a-deployment","title":"Exercise: Rolling Back a Deployment","text":"<p>Objective: Learn how to view the history of a Deployment and rollback to older revisions.</p> <p>Note: This exercise builds off the previous exercise: Using Deployments. If you have not, complete it first before continuing.</p> <p>1) Use the <code>rollout</code> command to view the <code>history</code> of the Deployment <code>deploy-example</code>. <pre><code>$ kubectl rollout history deployment deploy-example\n</code></pre> There should be two revisions. One for when the Deployment was first created, and another when the additional Labels were added. The number of revisions saved is based off of the <code>revisionHistoryLimit</code> attribute in the Deployment spec.</p> <p>2) Look at the details of a specific revision by passing the <code>--revision=&lt;revision number&gt;</code> flag. <pre><code>$ kubectl rollout history deployment deploy-example --revision=1\n$ kubectl rollout history deployment deploy-example --revision=2\n</code></pre> Viewing the specific revision will display a summary of the Pod Template.</p> <p>3) Choose to go back to revision <code>1</code> by using the <code>rollout undo</code> command. <pre><code>$ kubectl rollout undo deployment deploy-example --to-revision=1\n</code></pre> Tip: The <code>--to-revision</code> flag can be omitted if you wish to just go back to the previous configuration.</p> <p>4) Immediately watch the Pods. <pre><code>$ kubectl get pods --show-labels --watch\n</code></pre> They will cycle through rolling back to the previous revision.</p> <p>5) Describe the Deployment <code>deploy-example</code>. <pre><code>$ kubectl describe deployment deploy-example\n</code></pre> The events will describe the scaling back of the previous and switching over to the desired revision.</p> <p>Summary: Understanding how to use <code>rollout</code> command to both get a diff of the different revisions as well as be able to roll-back to a previously known good configuration is an important aspect of Deployments that cannot be left out.</p> <p>Clean Up Command <pre><code>kubectl delete deploy deploy-example\n</code></pre></p> <p>Back to Index</p>"},{"location":"kubernetes/workloads/#-","title":"---","text":""},{"location":"kubernetes/workloads/#daemonsets","title":"DaemonSets","text":"<p>DaemonSets ensure that all nodes matching certain criteria will run an instance of the supplied Pod.</p> <p>They bypass default scheduling mechanisms and restrictions, and are ideal for cluster wide services such as log forwarding, or health monitoring.</p>"},{"location":"kubernetes/workloads/#exercise-managing-daemonsets","title":"Exercise: Managing DaemonSets","text":"<p>Objective: Experience creating, updating, and rolling back a DaemonSet. Additionally delve into the process of how they are scheduled and how an update occurs.</p> <p>1) Create DaemonSet <code>ds-example</code> and pass the <code>--record</code> flag. Use the example yaml block below as a base, or use the manifest <code>manifests/ds-example.yaml</code> directly.</p> <p>manifests/ds-example.yaml <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ds-example\nspec:\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      nodeSelector:\n        nodeType: edge\n      containers:\n      - name: nginx\n        image: nginx:stable-alpine\n        ports:\n        - containerPort: 80\n</code></pre></p> <p>Command <pre><code>$ kubectl create -f manifests/ds-example.yaml --record\n</code></pre></p> <p>2) View the current DaemonSets. <pre><code>$ kubectl get daemonset\n</code></pre> As there are no matching nodes, no Pods should be scheduled.</p> <p>3) Label the <code>kind-control-plane</code> node with <code>nodeType=edge</code> <pre><code>$ kubectl label node kind-control-plane nodeType=edge\n</code></pre></p> <p>4) View the current DaemonSets once again. <pre><code>$ kubectl get daemonsets\n</code></pre> There should now be a single instance of the DaemonSet <code>ds-example</code> deployed.</p> <p>5) View the current Pods and display their labels with <code>--show-labels</code>. <pre><code>$ kubectl get pods --show-labels\n</code></pre> Note that the deployed Pod has a <code>controller-revision-hash</code> label. This is used like the <code>pod-template-hash</code> in a Deployment to track and allow for rollback functionality.</p> <p>6) Describing the DaemonSet will provide you with status information regarding it's Deployment cluster wide. <pre><code>$ kubectl describe ds ds-example\n</code></pre> Tip: <code>ds</code> can be substituted for <code>daemonset</code> when using <code>kubectl</code>.</p> <p>7) Update the DaemonSet by adding a few additional labels to the Pod Template and use the <code>--record</code> flag. <pre><code>$ kubectl apply -f manifests/ds-example.yaml --record\n  &lt; or &gt;\n$ kubectl edit ds ds-example --record\n</code></pre></p> <p>8) Watch the Pods and be sure to show the labels. <pre><code>$ kubectl get pods --show-labels --watch\n</code></pre> The old version of the DaemonSet will be phased out one at a time and instances of the new version will take its place. Similar to Deployments, DaemonSets have their own equivalent to a Deployment's <code>strategy</code> in the form of <code>updateStrategy</code>. The defaults are generally suitable, but other tuning options may be set. For reference, see the Updating DaemonSet Documentation.</p> <p>Summary: DaemonSets are usually used for important cluster-wide support services such as Pod Networking, Logging, or Monitoring. They differ from other workloads in that their scheduling bypasses normal mechanisms, and is centered around node placement. Like Deployments, they have their own <code>pod-template-hash</code> in the form of <code>controller-revision-hash</code> used for keeping track of Pod Template revisions and enabling rollback functionality.</p>"},{"location":"kubernetes/workloads/#optional-working-with-daemonset-revisions","title":"Optional: Working with DaemonSet Revisions","text":"<p>Objective: Explore using the <code>rollout</code> command to rollback to a specific version of a DaemonSet.</p> <p>Note: This exercise is functionally identical to the ExerciseRolling Back a Deployment. If you have completed that exercise, then this may be considered optional. Additionally, this exercise builds off the previous exercise Managing DaemonSets and it must be completed before continuing.</p> <p>1) Use the <code>rollout</code> command to view the <code>history</code> of the DaemonSet <code>ds-example</code> <pre><code>$ kubectl rollout history ds ds-example\n</code></pre> There should be two revisions. One for when the Deployment was first created, and another when the additional Labels were added. The number of revisions saved is based off of the <code>revisionHistoryLimit</code> attribute in the DaemonSet spec.</p> <p>2) Look at the details of a specific revision by passing the <code>--revision=&lt;revision number&gt;</code> flag. <pre><code>$ kubectl rollout history ds ds-example --revision=1\n$ kubectl rollout history ds ds-example --revision=2\n</code></pre> Viewing the specific revision will display the Pod Template.</p> <p>3) Choose to go back to revision <code>1</code> by using the <code>rollout undo</code> command. <pre><code>$ kubectl rollout undo ds ds-example --to-revision=1\n</code></pre> Tip: The <code>--to-revision</code> flag can be omitted if you wish to just go back to the previous configuration.</p> <p>4) Immediately watch the Pods. <pre><code>$ kubectl get pods --show-labels --watch\n</code></pre> They will cycle through rolling back to the previous revision.</p> <p>5) Describe the DaemonSet <code>ds-example</code>. <pre><code>$ kubectl describe ds ds-example\n</code></pre> The events will be sparse with a single host, however in an actual Deployment they will describe the status of updating the DaemonSet cluster wide, cycling through hosts one-by-one.</p> <p>Summary: Being able to use the <code>rollout</code> command with DaemonSets is import in scenarios where one may have to quickly go back to a previously known-good version. This becomes even more important for 'infrastructure' like services such as Pod Networking.</p> <p>Clean Up Command <pre><code>kubectl delete ds ds-example\n</code></pre></p> <p>Back to Index</p>"},{"location":"kubernetes/workloads/#-_1","title":"---","text":""},{"location":"kubernetes/workloads/cronjob/","title":"Jobs and CronJobs","text":"<p>The Job Controller ensures one or more Pods are executed and successfully terminate. Essentially a task executor that can be run in parallel.</p> <p>CronJobs are an extension of the Job Controller, and enable Jobs to be run on a schedule.</p>"},{"location":"kubernetes/workloads/cronjob/#exercise-creating-a-job","title":"Exercise: Creating a Job","text":"<p>Objective: Create a Kubernetes <code>Job</code> and work to understand how the Pods are managed with <code>completions</code> and <code>parallelism</code> directives.</p> <p>1) Create job <code>job-example</code> using the yaml below, or the manifest located at <code>manifests/job-example.yaml</code></p> <p>manifests/job-example.yaml <pre><code>apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-example\nspec:\n  backoffLimit: 4\n  completions: 4\n  parallelism: 2\n  template:\n    spec:\n      containers:\n      - name: hello\n        image: alpine:latest\n        command: [\"/bin/sh\", \"-c\"]\n        args: [\"echo hello from $HOSTNAME!\"]\n      restartPolicy: Never\n</code></pre></p> <p>Command <pre><code>$ kubectl create -f manifests/job-example.yaml\n</code></pre></p> <p>2) Watch the Pods as they are being created. <pre><code>$ kubectl get pods --show-labels --watch\n</code></pre> Only two Pods are being provisioned at a time; adhering to the <code>parallelism</code> attribute. This is done until the total number of <code>completions</code> is satisfied. Additionally, the Pods are labeled with <code>controller-uid</code>, this acts as a unique ID for that specific Job. </p> <p>When done, the Pods persist in a <code>Completed</code> state. They are not deleted after the Job is completed or failed.  This is intentional to better support troubleshooting.</p> <p>3) A summary of these events can be seen by describing the Job itself. <pre><code>$ kubectl describe job job-example\n</code></pre></p> <p>4) Delete the job. <pre><code>$ kubectl delete job job-example\n</code></pre></p> <p>5) View the Pods once more. <pre><code>$ kubectl get pods\n</code></pre> The Pods will now be deleted. They are cleaned up when the Job itself is removed.</p> <p>Summary: Jobs are fire and forget one off tasks, batch processing or as an executor for a workflow engine. They \"run to completion\" or terminate gracefully adhering to the <code>completions</code> and <code>parallelism</code> directives.</p>"},{"location":"kubernetes/workloads/cronjob/#exercise-scheduling-a-cronjob","title":"Exercise: Scheduling a CronJob","text":"<p>Objective: Create a CronJob based off a Job Template. Understand how the Jobs are generated and how to suspend a job in the event of a problem.</p> <p>1) Create CronJob <code>cronjob-example</code> based off the yaml below, or use the manifest <code>manifests/cronjob-example.yaml</code> It is configured to run the Job from the earlier example every minute, using the cron schedule <code>\"*/1 * * * *\"</code>. This schedule is UTC ONLY.</p> <p>manifests/cronjob-example.yaml <pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: cronjob-example\nspec:\n  schedule: \"*/1 * * * *\"\n  successfulJobsHistoryLimit: 2\n  failedJobsHistoryLimit: 1\n  jobTemplate:\n    spec:\n      completions: 4\n      parallelism: 2\n      template:\n        spec:\n          containers:\n          - name: hello\n            image: alpine:latest\n            command: [\"/bin/sh\", \"-c\"]\n            args: [\"echo hello from $HOSTNAME!\"]\n          restartPolicy: Never\n</code></pre></p> <p>Command <pre><code>$ kubectl create -f manifests/cronjob-example.yaml\n</code></pre></p> <p>2) Give it some time to run, and then list the Jobs. <pre><code>$ kubectl get jobs\n</code></pre> There should be at least one Job named in the format <code>&lt;cronjob-name&gt;-&lt;unix time stamp&gt;</code>. Note the timestamp of the oldest Job.</p> <p>3) Give it a few minutes and list the Jobs once again <pre><code>$ kubectl get jobs\n</code></pre> The oldest Job should have been removed. The CronJob controller will purge Jobs according to the <code>successfulJobHistoryLimit</code> and <code>failedJobHistoryLimit</code> attributes. In this case, it is retaining strictly the last 3 successful Jobs.</p> <p>4) Describe the CronJob <code>cronjob-example</code> <pre><code>$ kubectl describe CronJob cronjob-example\n</code></pre> The events will show the records of the creation and deletion of the Jobs.</p> <p>5) Edit the CronJob <code>cronjob-example</code> and locate the <code>Suspend</code> field. Then set it to true. <pre><code>$ kubectl edit CronJob cronjob-example\n</code></pre> This will prevent the cronjob from firing off any future events, and is useful to do to initially troubleshoot an issue without having to delete the CronJob directly.</p> <p>5) Delete the CronJob <pre><code>$ kubectl delete cronjob cronjob-example\n</code></pre> Deleting the CronJob WILL delete all child Jobs. Use <code>Suspend</code> to 'stop' the Job temporarily if attempting to troubleshoot.</p> <p>Summary: CronJobs are a useful extension of Jobs. They are great for backup or other day-to-day tasks, with the only caveat being they adhere to a UTC ONLY schedule.</p> <p>Clean Up Commands <pre><code>kubectl delete CronJob cronjob-example\n</code></pre></p>"},{"location":"kubernetes/workloads/cronjob/#helpful-resources","title":"Helpful Resources","text":"<ul> <li>Deployment Overview</li> <li>DaemonSet Overview</li> <li>StatefulSet Basics</li> <li>StatefulSet Overview</li> <li>Job Overview</li> </ul> <p>Back to Index</p>"},{"location":"kubernetes/workloads/daemonset/","title":"DaemonSets","text":"<p>DaemonSets ensure that all nodes matching certain criteria will run an instance of the supplied Pod.</p> <p>They bypass default scheduling mechanisms and restrictions, and are ideal for cluster wide services such as log forwarding, or health monitoring.</p>"},{"location":"kubernetes/workloads/daemonset/#exercise-managing-daemonsets","title":"Exercise: Managing DaemonSets","text":"<p>Objective: Experience creating, updating, and rolling back a DaemonSet. Additionally delve into the process of how they are scheduled and how an update occurs.</p> <p>1) Create DaemonSet <code>ds-example</code> and pass the <code>--record</code> flag. Use the example yaml block below as a base, or use the manifest <code>manifests/ds-example.yaml</code> directly.</p> <p>manifests/ds-example.yaml <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ds-example\nspec:\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      nodeSelector:\n        nodeType: edge\n      containers:\n      - name: nginx\n        image: nginx:stable-alpine\n        ports:\n        - containerPort: 80\n</code></pre></p> <p>Command <pre><code>$ kubectl create -f manifests/ds-example.yaml --record\n</code></pre></p> <p>2) View the current DaemonSets. <pre><code>$ kubectl get daemonset\n</code></pre> As there are no matching nodes, no Pods should be scheduled.</p> <p>3) Label the <code>kind-control-plane</code> node with <code>nodeType=edge</code> <pre><code>$ kubectl label node kind-control-plane nodeType=edge\n</code></pre></p> <p>4) View the current DaemonSets once again. <pre><code>$ kubectl get daemonsets\n</code></pre> There should now be a single instance of the DaemonSet <code>ds-example</code> deployed.</p> <p>5) View the current Pods and display their labels with <code>--show-labels</code>. <pre><code>$ kubectl get pods --show-labels\n</code></pre> Note that the deployed Pod has a <code>controller-revision-hash</code> label. This is used like the <code>pod-template-hash</code> in a Deployment to track and allow for rollback functionality.</p> <p>6) Describing the DaemonSet will provide you with status information regarding it's Deployment cluster wide. <pre><code>$ kubectl describe ds ds-example\n</code></pre> Tip: <code>ds</code> can be substituted for <code>daemonset</code> when using <code>kubectl</code>.</p> <p>7) Update the DaemonSet by adding a few additional labels to the Pod Template and use the <code>--record</code> flag. <pre><code>$ kubectl apply -f manifests/ds-example.yaml --record\n  &lt; or &gt;\n$ kubectl edit ds ds-example --record\n</code></pre></p> <p>8) Watch the Pods and be sure to show the labels. <pre><code>$ kubectl get pods --show-labels --watch\n</code></pre> The old version of the DaemonSet will be phased out one at a time and instances of the new version will take its place. Similar to Deployments, DaemonSets have their own equivalent to a Deployment's <code>strategy</code> in the form of <code>updateStrategy</code>. The defaults are generally suitable, but other tuning options may be set. For reference, see the Updating DaemonSet Documentation.</p> <p>Summary: DaemonSets are usually used for important cluster-wide support services such as Pod Networking, Logging, or Monitoring. They differ from other workloads in that their scheduling bypasses normal mechanisms, and is centered around node placement. Like Deployments, they have their own <code>pod-template-hash</code> in the form of <code>controller-revision-hash</code> used for keeping track of Pod Template revisions and enabling rollback functionality.</p>"},{"location":"kubernetes/workloads/daemonset/#optional-working-with-daemonset-revisions","title":"Optional: Working with DaemonSet Revisions","text":"<p>Objective: Explore using the <code>rollout</code> command to rollback to a specific version of a DaemonSet.</p> <p>Note: This exercise is functionally identical to the ExerciseRolling Back a Deployment. If you have completed that exercise, then this may be considered optional. Additionally, this exercise builds off the previous exercise Managing DaemonSets and it must be completed before continuing.</p> <p>1) Use the <code>rollout</code> command to view the <code>history</code> of the DaemonSet <code>ds-example</code> <pre><code>$ kubectl rollout history ds ds-example\n</code></pre> There should be two revisions. One for when the Deployment was first created, and another when the additional Labels were added. The number of revisions saved is based off of the <code>revisionHistoryLimit</code> attribute in the DaemonSet spec.</p> <p>2) Look at the details of a specific revision by passing the <code>--revision=&lt;revision number&gt;</code> flag. <pre><code>$ kubectl rollout history ds ds-example --revision=1\n$ kubectl rollout history ds ds-example --revision=2\n</code></pre> Viewing the specific revision will display the Pod Template.</p> <p>3) Choose to go back to revision <code>1</code> by using the <code>rollout undo</code> command. <pre><code>$ kubectl rollout undo ds ds-example --to-revision=1\n</code></pre> Tip: The <code>--to-revision</code> flag can be omitted if you wish to just go back to the previous configuration.</p> <p>4) Immediately watch the Pods. <pre><code>$ kubectl get pods --show-labels --watch\n</code></pre> They will cycle through rolling back to the previous revision.</p> <p>5) Describe the DaemonSet <code>ds-example</code>. <pre><code>$ kubectl describe ds ds-example\n</code></pre> The events will be sparse with a single host, however in an actual Deployment they will describe the status of updating the DaemonSet cluster wide, cycling through hosts one-by-one.</p> <p>Summary: Being able to use the <code>rollout</code> command with DaemonSets is import in scenarios where one may have to quickly go back to a previously known-good version. This becomes even more important for 'infrastructure' like services such as Pod Networking.</p> <p>Clean Up Command <pre><code>kubectl delete ds ds-example\n</code></pre></p>"},{"location":"kubernetes/workloads/job-cronjob/","title":"Jobs and CronJobs","text":""},{"location":"kubernetes/workloads/job-cronjob/#jobs","title":"Jobs","text":"<p>Working with Jobs is a bit different than working with Deployments. Create a Job using the following template:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: sleepy\nspec:\n  template:\n    spec:\n      containers:\n      - name: resting\n        image: busybox\n        command: [\"/bin/sleep\"] \n        args: [\"3\"]\n      restartPolicy: Never\nEOF\njob.batch/sleepy created\n</code></pre> <p>Check the Job</p> <pre><code>kubectl get job\nNAME COMPLETIONS DURATION AGE\nsleepy 0/1         3s     3s\n\nkubectl describe jobs.batch sleepy\nName:             sleepy\nNamespace:        default\nSelector:         batch.kubernetes.io/controller-uid=97bb2f4b-b7de-4a8a-bc9a-180ad55d4a84\nLabels:           batch.kubernetes.io/controller-uid=97bb2f4b-b7de-4a8a-bc9a-180ad55d4a84\n                  batch.kubernetes.io/job-name=sleepy\n                  controller-uid=97bb2f4b-b7de-4a8a-bc9a-180ad55d4a84\n                  job-name=sleepy\nAnnotations:      &lt;none&gt;\nParallelism:      1\nCompletions:      1\nCompletion Mode:  NonIndexed\nSuspend:          false\nBackoff Limit:    6\nStart Time:       Thu, 24 Oct 2024 00:41:56 +0200\nCompleted At:     Thu, 24 Oct 2024 00:42:06 +0200\nDuration:         10s\nPods Statuses:    0 Active (0 Ready) / 1 Succeeded / 0 Failed\n</code></pre> <p>Play around with <code>parallelism</code>, <code>activeDeadlineSeconds</code> (change the sleep parameter too) and <code>completions</code> number (delete the job in between retries), how does that affect the number of concurrent pods running? </p>"},{"location":"kubernetes/workloads/job-cronjob/#cronjobs","title":"CronJobs","text":"<p>Create the CronJob:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: sleepy\nspec:\n  schedule: \"*/2 * * * *\" #&lt;-- Add Linux style cronjob syntax\n  jobTemplate: #&lt;-- New jobTemplate and spec move\n    spec:\n      template: #&lt;-- This and following lines move\n        spec: #&lt;-- four spaces to the right\n          containers:\n          - name: resting\n            image: busybox\n            command: [\"/bin/sleep\"]\n            args: [\"5\"]\n          restartPolicy: Never\nEOF\n</code></pre> <p>Check the execution:</p> <pre><code>$ kubectl get cronjobs\nNAME     SCHEDULE      TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE\nsleepy   */2 * * * *   &lt;none&gt;     False     0        &lt;none&gt;          11s\n\n$ kubectl get jobs\nNAME     STATUS     COMPLETIONS   DURATION   AGE\nsleepy   Complete   1/1           10s        9m44s\n</code></pre>"},{"location":"kubernetes/workloads/pdb/","title":"PodDisruptionBudget","text":"<p>Here\u2019s a short lab to demonstrate the use of PodDisruptionBudget (PDB) in Kubernetes. This lab will guide you through creating a sample deployment and configuring a PDB to ensure that a minimum number of replicas remain available during disruptions.</p> <p>Lab: Using PodDisruptionBudget in Kubernetes</p> <p>Objective</p> <p>Learn how to configure and use a PodDisruptionBudget to protect the availability of your pods during voluntary disruptions.</p> <p>Prerequisites</p> <pre><code>\u2022   A running Kubernetes cluster\n\u2022   kubectl configured to interact with your cluster\n</code></pre> <p>Step 1: Create a Sample Deployment</p> <p>First, let\u2019s create a sample deployment with 3 replicas of an nginx pod.</p> <pre><code>1.  Create a file named nginx-deployment.yaml with the following content:\n</code></pre> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\nEOF\n</code></pre> <pre><code>2.  Apply the deployment file to your cluster:\n</code></pre> <pre><code>kubectl apply -f nginx-deployment.yaml\n</code></pre> <pre><code>3.  Verify that the pods are running:\n</code></pre> <pre><code>kubectl get pods -l app=nginx\n</code></pre> <p>Step 2: Define a PodDisruptionBudget (PDB)</p> <p>Now that we have a deployment with 3 replicas, let\u2019s create a PodDisruptionBudget to ensure at least 2 pods are available during disruptions.</p> <pre><code>1.  Create a file named nginx-pdb.yaml with the following content:\n</code></pre> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: nginx-pdb\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: nginx\nEOF\n</code></pre> <pre><code>2.  Apply the PDB to your cluster:\n</code></pre> <p>kubectl apply -f nginx-pdb.yaml</p> <pre><code>3.  Confirm that the PDB has been created:\n</code></pre> <p>kubectl get pdb</p> <p>Step 3: Test the PodDisruptionBudget</p> <p>Let\u2019s test the PDB by attempting to drain a node where one or more of the nginx pods are running. The PDB should prevent the node drain if it would cause more than one pod to go unavailable.</p> <pre><code>1.  Find a node where an nginx pod is running:\n</code></pre> <p>kubectl get pods -o wide -l app=nginx</p> <pre><code>2.  Attempt to drain that node:\n</code></pre> <p>kubectl drain  --ignore-daemonsets --delete-emptydir-data <p>Since we have a PDB in place, Kubernetes will prevent the drain if it cannot maintain the minimum number of available replicas specified in the PDB.</p> <pre><code>3.  After testing, you can uncordon the node:\n</code></pre> <p>kubectl uncordon  <p>Cleanup</p> <p>To remove the resources created in this lab, use the following commands:</p> <p>kubectl delete -f nginx-deployment.yaml kubectl delete -f nginx-pdb.yaml</p> <p>Summary</p> <p>In this lab, we configured a PodDisruptionBudget for a deployment to ensure a minimum number of replicas remain available during node disruptions. This feature is particularly useful for applications that need to maintain high availability during maintenance operations.</p> <p>This lab provides a basic understanding of how PDBs work and how they help in maintaining application uptime during voluntary disruptions.</p>"},{"location":"kubernetes/workloads/statefulsets/","title":"StatefulSets","text":"<p>The StatefulSet controller is tailored to managing Pods that must persist or maintain state. Pod identity including hostname, network, and storage can be considered persistent.</p> <p>They ensure persistence by making use of three things: * The StatefulSet controller enforcing predicable naming, and ordered provisioning/updating/deletion. * A headless service to provide a unique network identity. * A volume template to ensure stable per-instance storage.</p>"},{"location":"kubernetes/workloads/statefulsets/#exercise-managing-statefulsets","title":"Exercise: Managing StatefulSets","text":"<p>Objective: Create, update, and delete a <code>StatefulSet</code> to gain an understanding of how the StatefulSet lifecycle differs from other workloads with regards to updating, deleting and the provisioning of storage.</p> <p>1) Create StatefulSet <code>sts-example</code> using the yaml block below or the manifest <code>manifests/sts-example.yaml</code>.</p> <p>manifests/sts-example.yaml <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sts-example\nspec:\n  replicas: 3\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app: stateful\n  serviceName: app\n  updateStrategy:\n    type: OnDelete\n  template:\n    metadata:\n      labels:\n        app: stateful\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:stable-alpine\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n  volumeClaimTemplates:\n  - metadata:\n      name: www\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      storageClassName: standard\n      resources:\n        requests:\n          storage: 1Gi\n</code></pre></p> <p>Command <pre><code>$ kubectl create -f manifests/sts-example.yaml\n</code></pre></p> <p>2) Immediately watch the Pods being created. <pre><code>$ kubectl get pods --show-labels --watch\n</code></pre> Unlike Deployments or DaemonSets, the Pods of a StatefulSet are created one-by-one, going by their ordinal index. Meaning, <code>sts-example-0</code> will fully be provisioned before <code>sts-example-1</code> starts up. Additionally, take notice of the <code>controller-revision-hash</code> label. This serves the same purpose as the <code>controller-revision-hash</code> label in a DaemonSet or the <code>pod-template-hash</code> in a Deployment. It provides a means of tracking the revision of the Pod Template and enables rollback functionality.</p> <p>3) More information on the StatefulSet can be gleaned about the state of the StatefulSet by describing it. <pre><code>$ kubectl describe statefulset sts-example\n</code></pre> Within the events, notice that it is creating claims for volumes before each Pod is created.</p> <p>4) View the current Persistent Volume Claims. <pre><code>$ kubectl get pvc\n</code></pre> The StatefulSet controller creates a volume for each instance based off the <code>volumeClaimTemplate</code>. It prepends the volume name to the Pod name. e.g. <code>www-sts-example-0</code>.</p> <p>5) Update the StatefulSet's Pod Template and add a few additional labels. <pre><code>$ kubectl apply -f manifests/sts-example.yaml --record\n  &lt; or &gt;\n$ kubectl edit statefulset sts-example --record\n</code></pre></p> <p>6) Return to watching the Pods. <pre><code>$ kubectl get pods --show-labels\n</code></pre> None of the Pods are being updated to the new version of the Pod.</p> <p>7) Delete the <code>sts-example-2</code> Pod. <pre><code>$ kubectl delete pod sts-example-2\n</code></pre></p> <p>8) Immediately get the Pods. <pre><code>$ kubectl get pods --show-labels --watch\n</code></pre> The new <code>sts-example-2</code> Pod should be created with the new additional labels. The <code>OnDelete</code> Update Strategy will not spawn a new iteration of the Pod until the previous one was deleted. This allows for manual gating the update process for the StatefulSet.</p> <p>9) Update the StatefulSet and change the Update Strategy Type to <code>RollingUpdate</code>. <pre><code>$ kubectl apply -f manifests/sts-example.yaml --record\n  &lt; or &gt;\n$ kubectl edit statefulset sts-example --record\n</code></pre></p> <p>10) Immediately watch the Pods once again. <pre><code>$ kubectl get pods --show-labels --watch\n</code></pre> Note that the Pods are sequentially updated in descending order, or largest to smallest based on the Pod's ordinal index. This means that if <code>sts-example-2</code> was not updated already, it would be updated first, then <code>sts-example-1</code> and finally <code>sts-example-0</code>.</p> <p>11) Delete the StatefulSet <code>sts-example</code> <pre><code>$ kubectl delete statefulset sts-example\n</code></pre></p> <p>12) View the Persistent Volume Claims. <pre><code>$ kubectl get pvc\n</code></pre> Created PVCs are NOT garbage collected automatically when a StatefulSet is deleted. They must be reclaimed independently of the StatefulSet itself.</p> <p>13) Recreate the StatefulSet using the same manifest. <pre><code>$ kubectl create -f manifests/sts-example.yaml --record\n</code></pre></p> <p>14) View the Persistent Volume Claims again. <pre><code>$ kubectl get pvc\n</code></pre> Note that new PVCs were NOT provisioned. The StatefulSet controller assumes if the matching name is present, that PVC is intended to be used for the associated Pod.</p> <p>Summary: Like many applications where state must be taken into account, the planning and usage of StatefulSets requires forethought. The consistency brought by standard naming, ordered updates/deletes and templated storage does however make this task easier.</p>"},{"location":"kubernetes/workloads/statefulsets/#exercise-understanding-statefulset-network-identity","title":"Exercise: Understanding StatefulSet Network Identity","text":"<p>Objective: Create a \"headless service\" or a service without a <code>ClusterIP</code> (<code>ClusterIP=None</code>) for use with the StatefulSet <code>sts-example</code>, then explore how this enables consistent service discovery.</p> <p>1) Create the headless service <code>app</code> using the <code>app=stateful</code> selector from the yaml below or the manifest <code>manifests/service-sts-example.yaml</code>.</p> <p>manifests/service-sts-example.yaml <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: app\nspec:\n  clusterIP: None\n  selector:\n    app: stateful\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n</code></pre></p> <p>Command <pre><code>$ kubectl create -f manifests/service-sts-example.yaml\n</code></pre></p> <p>2) Describe the newly created service <pre><code>$ kubectl describe svc app\n</code></pre> Notice that it does not have a <code>clusterIP</code>, but does have the Pod Endpoints listed. Headless services are unique in this behavior.</p> <p>3) Query the DNS entry for the <code>app</code> service. <pre><code>$ kubectl exec sts-example-0 -- nslookup app.default.svc.cluster.local\n</code></pre> An A record will have been returned for each instance of the StatefulSet. Querying the service directly will do simple DNS round-robin load-balancing.</p> <p>4) Finally, query one of instances directly. <pre><code>$ kubectl exec sts-example-0 -- nslookup sts-example-1.app.default.svc.cluster.local\n</code></pre> This is a unique feature to StatefulSets. This allows for services to directly interact with a specific instance of a Pod. If the Pod is updated and obtains a new IP, the DNS record will immediately point to it enabling consistent service discovery.</p> <p>Summary: StatefulSet service discovery is unique within Kubernetes in that it augments a headless service (A service without a unique <code>ClusterIP</code>) to provide a consistent mapping to the individual Pods. These mappings take the form of an A record in format of: <code>&lt;StatefulSet Name&gt;-&lt;ordinal&gt;.&lt;service name&gt;.&lt;namespace&gt;.svc.cluster.local</code> and can be used consistently throughout other Workloads.</p> <p>Clean Up Command <pre><code>kubectl delete svc app\nkubectl delete statefulset sts-example\nkubectl delete pvc www-sts-example-0 www-sts-example-1 www-sts-example-2\n</code></pre></p> <p>Back to Index</p>"},{"location":"observability/","title":"Observability in Kubernetes","text":""},{"location":"observability/#outcomes","title":"Outcomes","text":"<ul> <li>Understand the importance of observability in Kubernetes.</li> <li>Learn how to monitor and log Kubernetes clusters and applications.</li> <li>Understand how to use Prometheus and Grafana for metrics.</li> <li>Understand how to use Elasticsearch, Fluentd, and Kibana (EFK) for logging.</li> <li>Understand how to implement tracing using Jaeger.</li> </ul>"},{"location":"observability/#what-is-observability","title":"What is Observability?","text":"<p>Observability is a critical aspect of running modern Kubernetes applications. It allows teams to monitor, log, and trace the behavior of their clusters and workloads. In Kubernetes, observability encompasses three primary pillars:</p> <ol> <li>Metrics: Quantitative data that provides insights into the performance of the cluster and applications.</li> <li>Logs: Records of events, including errors and status changes, which help diagnose issues.</li> <li>Tracing: A way to track requests as they traverse through different services in the cluster, identifying bottlenecks or failures.</li> </ol> <p>Observability tools in Kubernetes provide visibility into system health, enabling proactive incident response and performance optimization.</p>"},{"location":"observability/#how-to-implement-observability-in-kubernetes","title":"How to Implement Observability in Kubernetes","text":""},{"location":"observability/#step-1-metrics-collection-with-prometheus","title":"Step 1: Metrics Collection with Prometheus","text":"<p>Prometheus is a popular open-source tool for gathering metrics in Kubernetes. It scrapes metrics from various services, stores them, and allows querying with PromQL.</p>"},{"location":"observability/#example-deploying-prometheus-with-helm","title":"Example: Deploying Prometheus with Helm","text":"<p>Using Helm, you can easily deploy Prometheus to your Kubernetes cluster:</p> <pre><code>helm install prometheus stable/prometheus\n</code></pre> <p>Prometheus can scrape metrics from various Kubernetes components like kubelet, kube-apiserver, and applications that expose metrics.</p>"},{"location":"observability/#step-2-visualizing-metrics-with-grafana","title":"Step 2: Visualizing Metrics with Grafana","text":"<p>Grafana is used for creating dashboards to visualize metrics gathered by Prometheus. It integrates seamlessly with Prometheus and supports various other data sources.</p>"},{"location":"observability/#example-deploying-grafana-with-prometheus","title":"Example: Deploying Grafana with Prometheus","text":"<pre><code>helm install grafana stable/grafana\n</code></pre> <p>Once installed, you can access Grafana via its UI and create dashboards to monitor cluster and application performance.</p> <ol> <li>Add Prometheus as a data source in Grafana.</li> <li>Use built-in or custom dashboards to visualize metrics.</li> </ol>"},{"location":"observability/#step-3-centralized-logging-with-elasticsearch-fluentd-and-kibana-efk","title":"Step 3: Centralized Logging with Elasticsearch, Fluentd, and Kibana (EFK)","text":"<p>Logs are critical for troubleshooting and understanding system behavior. In Kubernetes, the EFK stack (Elasticsearch, Fluentd, Kibana) is commonly used for centralized logging.</p> <ul> <li>Fluentd collects logs from containers and ships them to Elasticsearch.</li> <li>Elasticsearch stores the logs in a distributed database.</li> <li>Kibana provides a web UI to visualize and query logs.</li> </ul>"},{"location":"observability/#example-deploying-the-efk-stack","title":"Example: Deploying the EFK Stack","text":"<p>You can deploy the entire EFK stack using Helm or other tools. Here's a high-level guide:</p> <ol> <li> <p>Fluentd: Collects logs from all containers.    <pre><code>helm install fluentd stable/fluentd\n</code></pre></p> </li> <li> <p>Elasticsearch: Stores logs for fast retrieval.    <pre><code>helm install elasticsearch stable/elasticsearch\n</code></pre></p> </li> <li> <p>Kibana: Visualizes logs for easy troubleshooting.    <pre><code>helm install kibana stable/kibana\n</code></pre></p> </li> </ol> <p>Once deployed, you can access the Kibana UI to search and filter logs.</p>"},{"location":"observability/#step-4-tracing-with-jaeger","title":"Step 4: Tracing with Jaeger","text":"<p>Jaeger is an open-source tool for distributed tracing. It helps track requests as they move through multiple services in your Kubernetes environment, making it easier to pinpoint bottlenecks or service failures.</p>"},{"location":"observability/#example-deploying-jaeger-for-tracing","title":"Example: Deploying Jaeger for Tracing","text":"<p>You can deploy Jaeger in your cluster using Helm:</p> <pre><code>helm install jaeger stable/jaeger\n</code></pre> <p>Jaeger will collect traces from your applications, enabling you to view traces through its UI, which helps in understanding the path a request takes through your system.</p>"},{"location":"observability/#best-practices-for-observability","title":"Best Practices for Observability","text":"<ol> <li>Monitor all components: Ensure that you are collecting metrics from both the Kubernetes control plane and your applications.</li> <li>Set up alerting: Use Prometheus Alertmanager to notify your team when critical thresholds are breached.</li> <li>Use logs and metrics together: Logs provide context to metrics, helping to pinpoint root causes of issues.</li> <li>Automate observability deployments: Use tools like Helm and GitOps to automate the deployment and management of observability components.</li> </ol>"},{"location":"observability/#summary","title":"Summary","text":"<p>Observability in Kubernetes involves monitoring, logging, and tracing the behavior of clusters and workloads. By leveraging tools like Prometheus for metrics, Grafana for visualization, EFK for logging, and Jaeger for tracing, you can gain deep insights into your system's health and performance. Implementing a comprehensive observability strategy is essential for maintaining the reliability and performance of your Kubernetes applications.</p>"},{"location":"observability/logging/","title":"Logging","text":""},{"location":"observability/logging/#outcomes","title":"Outcomes","text":"<ul> <li>Understand the importance of centralized logging in Kubernetes.</li> <li>Learn how to deploy the Elasticsearch, Fluentd, and Kibana (EFK) stack for logging.</li> <li>Understand how to collect logs from Kubernetes clusters and applications.</li> <li>Learn how to view and analyze logs using Kibana.</li> <li>Understand best practices for managing logs in Kubernetes.</li> </ul>"},{"location":"observability/logging/#what-is-logging-in-kubernetes","title":"What is Logging in Kubernetes?","text":"<p>Logging is essential for understanding the behavior and performance of applications running in Kubernetes. In Kubernetes, each component and container generates logs that provide information about system performance, errors, and events. Centralized logging solutions, such as the Elasticsearch, Fluentd, Kibana (EFK) stack, help collect, store, and visualize logs across the entire Kubernetes cluster.</p>"},{"location":"observability/logging/#why-centralized-logging-is-important","title":"Why Centralized Logging is Important:","text":"<ul> <li>Easier troubleshooting: Centralized logs provide a single location to analyze logs from all components and containers.</li> <li>Long-term storage: Kubernetes pods are ephemeral, so logs must be stored outside the pod lifecycle.</li> <li>Searchable and organized: Tools like Elasticsearch allow fast, full-text search across logs.</li> <li>Visual insights: Kibana provides a graphical interface to analyze trends and patterns in logs.</li> </ul>"},{"location":"observability/logging/#how-to-set-up-centralized-logging-with-the-efk-stack","title":"How to Set Up Centralized Logging with the EFK Stack","text":"<p>The EFK stack (Elasticsearch, Fluentd, and Kibana) is commonly used for centralized logging in Kubernetes. Each component plays a specific role: - Fluentd: Collects and forwards logs to Elasticsearch. - Elasticsearch: Stores and indexes logs for fast retrieval. - Kibana: Provides a UI to visualize and search logs.</p>"},{"location":"observability/logging/#step-1-deploying-elasticsearch","title":"Step 1: Deploying Elasticsearch","text":"<p>Elasticsearch stores logs collected by Fluentd. It\u2019s a distributed, RESTful search engine that allows fast full-text searches.</p> <p>To deploy Elasticsearch using Helm:</p> <ol> <li> <p>Add the Helm repository:    <pre><code>helm repo add elastic https://helm.elastic.co\n</code></pre></p> </li> <li> <p>Install Elasticsearch:    <pre><code>helm install elasticsearch elastic/elasticsearch\n</code></pre></p> </li> </ol> <p>This will create an Elasticsearch cluster that is ready to receive logs from Fluentd.</p>"},{"location":"observability/logging/#step-2-deploying-fluentd","title":"Step 2: Deploying Fluentd","text":"<p>Fluentd is responsible for collecting logs from Kubernetes pods and nodes, formatting them, and sending them to Elasticsearch.</p> <p>To deploy Fluentd using Helm:</p> <ol> <li> <p>Add the Helm repository:    <pre><code>helm repo add fluent https://fluent.github.io/helm-charts\n</code></pre></p> </li> <li> <p>Install Fluentd:    <pre><code>helm install fluentd fluent/fluentd\n</code></pre></p> </li> </ol> <p>By default, Fluentd will collect logs from all containers and nodes in your Kubernetes cluster and forward them to Elasticsearch.</p>"},{"location":"observability/logging/#step-3-deploying-kibana","title":"Step 3: Deploying Kibana","text":"<p>Kibana is a web UI that allows you to query and visualize the logs stored in Elasticsearch. It provides charts, graphs, and search capabilities that help in analyzing logs.</p> <p>To deploy Kibana using Helm:</p> <ol> <li> <p>Install Kibana:    <pre><code>helm install kibana elastic/kibana\n</code></pre></p> </li> <li> <p>Access Kibana via port forwarding:    <pre><code>kubectl port-forward svc/kibana-kibana 5601:5601\n</code></pre></p> </li> </ol> <p>Once deployed, Kibana will be accessible at <code>http://localhost:5601</code>, where you can visualize logs from your Kubernetes cluster.</p>"},{"location":"observability/logging/#collecting-logs-from-kubernetes-pods","title":"Collecting Logs from Kubernetes Pods","text":"<p>Fluentd collects logs from all pods by default in Kubernetes. If your application is logging to <code>stdout</code> or <code>stderr</code>, Fluentd automatically collects and forwards these logs to Elasticsearch. However, you can customize the logging behavior by defining <code>ConfigMaps</code> or using custom Fluentd filters.</p>"},{"location":"observability/logging/#example-custom-fluentd-configmap","title":"Example: Custom Fluentd ConfigMap","text":"<p>If you need to apply custom filtering or routing rules for logs, you can configure Fluentd with a <code>ConfigMap</code>. Below is an example of a custom Fluentd configuration:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: fluentd-config\n  namespace: kube-system\ndata:\n  fluent.conf: |\n    &lt;source&gt;\n      @type tail\n      path /var/log/containers/*.log\n      pos_file /var/log/fluentd-containers.log.pos\n      tag kube.*\n      format json\n    &lt;/source&gt;\n\n    &lt;filter kube.**&gt;\n      @type kubernetes_metadata\n    &lt;/filter&gt;\n\n    &lt;match kube.**&gt;\n      @type elasticsearch\n      host elasticsearch.default.svc.cluster.local\n      port 9200\n      logstash_format true\n    &lt;/match&gt;\n</code></pre> <p>This configuration collects logs from all containers and forwards them to Elasticsearch with Kubernetes metadata, which is useful for querying logs based on namespaces, pod names, or labels.</p>"},{"location":"observability/logging/#viewing-and-analyzing-logs-in-kibana","title":"Viewing and Analyzing Logs in Kibana","text":"<p>Once logs are collected and indexed by Elasticsearch, Kibana provides powerful tools to search, filter, and visualize logs.</p>"},{"location":"observability/logging/#step-1-creating-an-index-pattern-in-kibana","title":"Step 1: Creating an Index Pattern in Kibana","text":"<p>To start analyzing logs in Kibana, create an index pattern that matches the log indices stored in Elasticsearch:</p> <ol> <li>Open Kibana and go to Management &gt; Index Patterns.</li> <li>Create a new index pattern that matches the log indices (e.g., <code>fluentd-*</code>).</li> <li>Select a timestamp field (such as <code>@timestamp</code>) for filtering logs by time.</li> </ol>"},{"location":"observability/logging/#step-2-searching-logs-in-kibana","title":"Step 2: Searching Logs in Kibana","text":"<p>You can search logs in Kibana using its search bar. For example, to find logs for a specific pod:</p> <pre><code>kubernetes.pod_name: \"my-app-pod\"\n</code></pre> <p>You can also use the Discover tab to search for logs based on pod labels, namespaces, or log content.</p>"},{"location":"observability/logging/#step-3-visualizing-logs-with-kibana-dashboards","title":"Step 3: Visualizing Logs with Kibana Dashboards","text":"<p>Kibana allows you to create dashboards that provide visual representations of logs over time. You can create visualizations such as:</p> <ul> <li>Log counts over time.</li> <li>Error trends across different services.</li> <li>Logs grouped by pod or namespace.</li> </ul> <p>These dashboards help in identifying issues and trends quickly.</p>"},{"location":"observability/logging/#best-practices-for-logging-in-kubernetes","title":"Best Practices for Logging in Kubernetes","text":"<ol> <li>Log to stdout/stderr: Ensure that your applications log to <code>stdout</code> and <code>stderr</code> so that logs can be collected by Fluentd.</li> <li>Limit log verbosity: Avoid excessive logging to reduce the volume of logs and ensure relevant information is captured.</li> <li>Retain logs appropriately: Configure Elasticsearch for appropriate log retention policies to avoid excessive storage usage.</li> <li>Monitor log storage: Ensure that Elasticsearch has enough resources for storing and indexing logs, especially in large clusters.</li> <li>Use custom filters: Apply Fluentd filters to extract meaningful information from logs and discard unnecessary data.</li> </ol>"},{"location":"observability/logging/#summary","title":"Summary","text":"<p>Centralized logging in Kubernetes is crucial for effective troubleshooting and monitoring. By deploying the EFK stack (Elasticsearch, Fluentd, Kibana), you can collect logs from all containers and nodes, store them efficiently, and visualize them through Kibana. This provides valuable insights into the behavior of your applications and helps identify and resolve issues quickly. Implementing best practices for logging will ensure that your Kubernetes environment remains performant and manageable.</p>"},{"location":"observability/prometheus/","title":"Prometheus","text":""},{"location":"observability/prometheus/#outcomes","title":"Outcomes","text":"<ul> <li>Understand what Prometheus is.</li> <li>Learn how to deploy Prometheus in a Kubernetes cluster.</li> <li>Understand how to scrape metrics from Kubernetes components and applications.</li> <li>Learn how to set up alerting with Prometheus Alertmanager.</li> <li>Understand how to integrate Prometheus with Grafana for visualization.</li> </ul>"},{"location":"observability/prometheus/#what-is-prometheus","title":"What is Prometheus?","text":"<p>Prometheus is an open-source monitoring and alerting toolkit designed specifically for reliability and scalability. It collects metrics from your applications, services, and systems, stores them in a time-series database, and provides a powerful query language (PromQL) for analyzing and visualizing the metrics.</p> <p>Prometheus is widely used in Kubernetes environments for monitoring because it can natively scrape metrics from Kubernetes components like kubelet, kube-apiserver, and user-defined applications.</p>"},{"location":"observability/prometheus/#key-benefits-of-prometheus","title":"Key Benefits of Prometheus:","text":"<ul> <li>Efficient metrics collection: Scrapes metrics from any HTTP endpoint that exposes them.</li> <li>Time-series database: Stores metrics efficiently for historical analysis.</li> <li>Alerting: Supports threshold-based alerting using Prometheus Alertmanager.</li> <li>Flexible querying: PromQL allows flexible, precise queries to analyze metrics.</li> <li>Scalability: Prometheus is horizontally scalable by using a federation model for large environments.</li> </ul>"},{"location":"observability/prometheus/#how-to-deploy-prometheus-in-kubernetes","title":"How to Deploy Prometheus in Kubernetes","text":""},{"location":"observability/prometheus/#step-1-deploy-prometheus-using-helm","title":"Step 1: Deploy Prometheus using Helm","text":"<p>You can deploy Prometheus using the Helm chart, which simplifies installation and configuration in a Kubernetes environment.</p> <ol> <li> <p>Add the stable Helm chart repository:    <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n</code></pre></p> </li> <li> <p>Install Prometheus with Helm:    <pre><code>helm install prometheus prometheus-community/prometheus\n</code></pre></p> </li> </ol> <p>This will install Prometheus with a default configuration in your Kubernetes cluster.</p>"},{"location":"observability/prometheus/#step-2-accessing-prometheus","title":"Step 2: Accessing Prometheus","text":"<p>To access Prometheus, you can forward the Prometheus server port to your local machine:</p> <pre><code>kubectl port-forward deploy/prometheus-server 9090\n</code></pre> <p>Once the port forwarding is active, you can open <code>http://localhost:9090</code> in your browser to access the Prometheus UI.</p>"},{"location":"observability/prometheus/#scraping-metrics-from-kubernetes","title":"Scraping Metrics from Kubernetes","text":"<p>Prometheus is often used to scrape metrics from Kubernetes components such as kubelet, kube-apiserver, and cAdvisor. Kubernetes exposes these metrics through its built-in <code>/metrics</code> endpoints.</p>"},{"location":"observability/prometheus/#example-scraping-kube-apiserver-metrics","title":"Example: Scraping kube-apiserver Metrics","text":"<p>To scrape metrics from the Kubernetes API server, you need to add a scrape configuration to the Prometheus configuration file (<code>prometheus.yml</code>):</p> <pre><code>scrape_configs:\n  - job_name: 'kube-apiserver'\n    static_configs:\n      - targets: ['kube-apiserver.default.svc.cluster.local:443']\n    scheme: https\n</code></pre> <p>Prometheus will scrape the kube-apiserver\u2019s <code>/metrics</code> endpoint for metrics on service requests, API usage, and latency.</p>"},{"location":"observability/prometheus/#monitoring-custom-applications","title":"Monitoring Custom Applications","text":"<p>To monitor custom applications, ensure your application exposes metrics in a Prometheus-compatible format. This can be done with libraries such as Prometheus client libraries for different programming languages (e.g., Go, Java, Python, etc.).</p>"},{"location":"observability/prometheus/#example-exposing-metrics-in-a-custom-application-go","title":"Example: Exposing Metrics in a Custom Application (Go)","text":"<pre><code>package main\n\nimport (\n    \"github.com/prometheus/client_golang/prometheus\"\n    \"github.com/prometheus/client_golang/prometheus/promhttp\"\n    \"net/http\"\n)\n\nvar requestCount = prometheus.NewCounter(\n    prometheus.CounterOpts{\n        Name: \"http_requests_total\",\n        Help: \"Number of HTTP requests processed.\",\n    },\n)\n\nfunc init() {\n    prometheus.MustRegister(requestCount)\n}\n\nfunc handler(w http.ResponseWriter, r *http.Request) {\n    requestCount.Inc()\n    w.Write([]byte(\"Hello, Prometheus!\"))\n}\n\nfunc main() {\n    http.Handle(\"/metrics\", promhttp.Handler())\n    http.HandleFunc(\"/\", handler)\n    http.ListenAndServe(\":8080\", nil)\n}\n</code></pre> <p>After deploying this application, you can configure Prometheus to scrape metrics from the app by adding a new job in <code>prometheus.yml</code>:</p> <pre><code>scrape_configs:\n  - job_name: 'custom-app'\n    static_configs:\n      - targets: ['custom-app-service:8080']\n</code></pre>"},{"location":"observability/prometheus/#setting-up-alerting-with-prometheus-alertmanager","title":"Setting Up Alerting with Prometheus Alertmanager","text":"<p>Prometheus is commonly used with Alertmanager to send alerts when metrics meet certain thresholds. Alertmanager can send notifications through various channels, such as email, Slack, PagerDuty, and more.</p>"},{"location":"observability/prometheus/#step-1-deploy-alertmanager-with-prometheus","title":"Step 1: Deploy Alertmanager with Prometheus","text":"<p>If you installed Prometheus using Helm, Alertmanager should already be deployed. Otherwise, you can deploy it separately:</p> <pre><code>helm install alertmanager prometheus-community/alertmanager\n</code></pre>"},{"location":"observability/prometheus/#step-2-configure-alerts-in-prometheus","title":"Step 2: Configure Alerts in Prometheus","text":"<p>You can define alerting rules in the Prometheus configuration file (<code>prometheus.yml</code>):</p> <pre><code>groups:\n- name: example-alerts\n  rules:\n  - alert: HighRequestLatency\n    expr: http_request_duration_seconds{job=\"custom-app\"} &gt; 0.5\n    for: 5m\n    labels:\n      severity: \"warning\"\n    annotations:\n      summary: \"High request latency detected\"\n</code></pre> <p>This rule triggers an alert if the request latency exceeds 0.5 seconds for more than 5 minutes.</p>"},{"location":"observability/prometheus/#step-3-sending-alerts-to-alertmanager","title":"Step 3: Sending Alerts to Alertmanager","text":"<p>Configure Prometheus to send alerts to Alertmanager:</p> <pre><code>alerting:\n  alertmanagers:\n  - static_configs:\n    - targets:\n      - 'alertmanager:9093'\n</code></pre> <p>Once this configuration is applied, any triggered alerts will be forwarded to Alertmanager, where they can be routed to specific notification channels.</p>"},{"location":"observability/prometheus/#visualizing-prometheus-metrics-with-grafana","title":"Visualizing Prometheus Metrics with Grafana","text":"<p>Grafana is a powerful tool for visualizing metrics collected by Prometheus. You can easily create dashboards and monitor your Kubernetes cluster and applications.</p>"},{"location":"observability/prometheus/#step-1-add-prometheus-as-a-data-source-in-grafana","title":"Step 1: Add Prometheus as a Data Source in Grafana","text":"<ol> <li> <p>Access Grafana using port-forwarding or a load balancer.    <pre><code>kubectl port-forward deploy/grafana 3000\n</code></pre></p> </li> <li> <p>Open <code>http://localhost:3000</code> and log into the Grafana UI.</p> </li> <li>Navigate to Configuration &gt; Data Sources and add Prometheus as a new data source. Set the URL to the Prometheus service (e.g., <code>http://prometheus-server</code>).</li> </ol>"},{"location":"observability/prometheus/#step-2-create-dashboards","title":"Step 2: Create Dashboards","text":"<p>Once Prometheus is added as a data source, you can create dashboards using PromQL queries to monitor different aspects of your Kubernetes environment or applications.</p>"},{"location":"observability/prometheus/#best-practices-for-using-prometheus","title":"Best Practices for Using Prometheus","text":"<ol> <li>Scrape frequently used metrics: Focus on essential metrics like CPU, memory, disk usage, and network traffic.</li> <li>Set up efficient alerting: Use Alertmanager to route important alerts to the appropriate team, ensuring a timely response to critical issues.</li> <li>Use dashboards for monitoring: Integrate Prometheus with Grafana to visualize and analyze metrics in real-time.</li> <li>Scale Prometheus: For larger environments, consider scaling Prometheus by using federation or sharding strategies.</li> </ol>"},{"location":"observability/prometheus/#summary","title":"Summary","text":"<p>Prometheus is a powerful and flexible monitoring tool that is well-suited for Kubernetes environments. By collecting and analyzing metrics, setting up alerts, and integrating with Grafana, Prometheus helps ensure the reliability and performance of your cluster and applications. Prometheus\u2019s scalability and ease of integration with other tools like Alertmanager and Grafana make it an essential tool in any Kubernetes monitoring stack.</p>"},{"location":"openshift/","title":"Openshift","text":"<p>This is the openshift module. It contains the following sections:</p> <ul> <li>Installation</li> <li>Command line</li> <li>Configuration</li> <li>Usage</li> <li>Development</li> </ul>"},{"location":"openshift/configuration/","title":"Openshift Configuration","text":""},{"location":"openshift/development/","title":"Development with Openshift","text":""},{"location":"openshift/installation/","title":"Openshift Installation","text":""},{"location":"openshift/oc/","title":"Openshift CLI","text":""},{"location":"openshift/oc/#outcomes","title":"Outcomes","text":"<ul> <li>Understand how to use the Openshift CLI</li> <li>Understand how to use the Openshift CLI to create and manage resources</li> <li>Differences between the Openshift CLI and the Kubernetes CLI</li> </ul>"},{"location":"openshift/usage/","title":"Openshift usage","text":""},{"location":"references/CONTRIBUTORS/","title":"Contributors","text":""},{"location":"references/CONTRIBUTORS/#maintainers","title":"Maintainers","text":"<p>Alessandro Vozza (@ams0)</p>"},{"location":"references/CONTRIBUTORS/#contributors_1","title":"Contributors","text":"<p>Luca Berton</p>"},{"location":"references/RESOURCES/","title":"References","text":"<ul> <li>Awesome Cloud Native Trainings</li> <li>The Kubernetes Learning Resources List</li> </ul>"},{"location":"security/","title":"Security","text":"<p>This document outlines the security best practices, examples, and configurations for securing Kubernetes deployments in an enterprise environment.</p>"},{"location":"security/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Service Accounts</li> <li>Role-Based Access Control (RBAC)</li> <li>Pod Security</li> <li>Network Policies</li> <li>Secrets Management</li> <li>Auditing and Monitoring</li> <li>Best Practices</li> </ul>"},{"location":"security/#introduction","title":"Introduction","text":"<p>Kubernetes security is essential to maintaining a secure and reliable cluster environment. This document covers important security mechanisms such as RBAC, Pod security, network policies, and secrets management that you should consider when deploying Kubernetes in production environments.</p>"},{"location":"security/#auditing-and-monitoring","title":"Auditing and Monitoring","text":"<p>Implementing an auditing and monitoring solution helps in tracking events and identifying suspicious activities. Tools like Prometheus, Grafana, and Falco can be used for real-time monitoring of your Kubernetes cluster.</p> <ul> <li>Enable audit logging: Capture detailed events occurring within your cluster.</li> <li>Monitor logs: Regularly inspect logs to identify potential security incidents.</li> <li>Alerting: Configure alerts to notify your team of critical events.</li> </ul>"},{"location":"security/#best-practices","title":"Best Practices","text":"<ul> <li>Run containers as non-root: Always configure your containers to run as a non-root user to limit privileges.</li> <li>Enable RBAC: Use Role-Based Access Control to define granular access to Kubernetes resources.</li> <li>Use network policies: Restrict pod-to-pod communication using Network Policies to reduce the attack surface.</li> <li>Encrypt Secrets: Ensure that Kubernetes Secrets are encrypted both in transit and at rest.</li> <li>Regular updates: Keep your Kubernetes and container runtime up to date with the latest security patches.</li> <li>Monitor your cluster: Implement tools like Falco, Prometheus, and Grafana to continuously monitor your cluster.</li> </ul>"},{"location":"security/network-policies/","title":"Network Policies in Kubernetes","text":""},{"location":"security/network-policies/#introduction","title":"Introduction","text":"<p>Network Policies in Kubernetes define how pods can communicate with each other and with network endpoints. They control the flow of traffic at the IP level, enhancing cluster security by limiting access to sensitive components.</p>"},{"location":"security/network-policies/#best-practices-for-network-policies","title":"Best Practices for Network Policies","text":"<ol> <li>Deny by default: Implement policies that deny all traffic by default, then selectively allow needed connections.</li> <li>Limit pod communication: Ensure that only necessary pods can communicate with each other.</li> <li>Use labels effectively: Utilize Kubernetes labels to apply network policies to specific pod groups.</li> <li>Apply ingress and egress policies: Define both ingress and egress policies for more control over the traffic.</li> </ol>"},{"location":"security/network-policies/#example-deny-all-traffic","title":"Example: Deny All Traffic","text":"<p>This network policy denies all incoming and outgoing traffic to and from the pods in the <code>default</code> namespace:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-all\n  namespace: default\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n</code></pre>"},{"location":"security/network-policies/#example-allowing-traffic-from-specific-pods","title":"Example: Allowing Traffic from Specific Pods","text":"<p>The following example allows traffic only from pods with the label role=frontend to pods with the label role=db on port 3306 (MySQL):</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend-db\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          role: frontend\n    ports:\n    - protocol: TCP\n      port: 3306\n</code></pre>"},{"location":"security/network-policies/#example-egress-network-policy","title":"Example: Egress Network Policy","text":"<p>The following policy allows outgoing traffic to the internet only on port 443 (HTTPS):</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-egress-internet\n  namespace: default\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 0.0.0.0/0\n    ports:\n    - protocol: TCP\n      port: 443\n</code></pre>"},{"location":"security/network-policies/#monitoring-network-policies","title":"Monitoring Network Policies","text":"<p>Monitoring traffic flow and policy effectiveness is crucial for debugging. Use tools such as Calico, Weave, or Cilium to manage and visualize network traffic and policy enforcement.</p>"},{"location":"security/pod-security/","title":"Pod Security Policies in Kubernetes","text":""},{"location":"security/pod-security/#introduction","title":"Introduction","text":"<p>Pod Security Policies (PSPs) control sensitive aspects of pod specification to ensure secure operations. With PSPs, cluster administrators can define conditions that pods must meet, such as running as a non-root user, disallowing privileged containers, and limiting access to host resources.</p>"},{"location":"security/pod-security/#best-practices-for-pod-security","title":"Best Practices for Pod Security","text":"<ol> <li>Run containers as non-root: Avoid running containers with root privileges to limit the attack surface.</li> <li>Disable privileged containers: Privileged containers should be disabled unless explicitly required.</li> <li>Control capabilities: Minimize the capabilities assigned to containers.</li> <li>Limit volume types: Restrict the volume types to necessary ones, such as <code>ConfigMap</code> or <code>PersistentVolumeClaim</code>.</li> <li>Restrict host namespaces: Avoid allowing access to host PID, network, and IPC namespaces.</li> </ol>"},{"location":"security/pod-security/#example-pod-security-policy","title":"Example: Pod Security Policy","text":"<p>The following Pod Security Policy restricts running privileged containers and ensures pods run as non-root users:</p> <pre><code>apiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n  name: restricted\nspec:\n  privileged: false\n  runAsUser:\n    rule: MustRunAsNonRoot\n  fsGroup:\n    rule: RunAsAny\n  volumes:\n    - 'configMap'\n    - 'emptyDir'\n    - 'secret'\n    - 'persistentVolumeClaim'\n  seLinux:\n    rule: RunAsAny\n  supplementalGroups:\n    rule: RunAsAny\n</code></pre>"},{"location":"security/pod-security/#enforcing-pod-security","title":"Enforcing Pod Security","text":"<p>To apply Pod Security Policies, create a Role or ClusterRole that grants access to use the PSP:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: mynamespace\n  name: use-psp\nrules:\n- apiGroups: [\"policy\"]\n  resources: [\"podsecuritypolicies\"]\n  verbs: [\"use\"]\n  resourceNames: [\"restricted\"]\n</code></pre> <p>Then, bind this role to a user or group using a RoleBinding or ClusterRoleBinding.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: psp-rolebinding\n  namespace: mynamespace\nsubjects:\n- kind: User\n  name: user1\nroleRef:\n  kind: Role\n  name: use-psp\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Deprecation</p> <p>PodSecurityPolicy is deprecated in Kubernetes 1.21 and will be removed in future releases. Alternatives like Open Policy Agent (OPA) or Pod Security Admission (PSA) are recommended for enforcing pod security in future versions of Kubernetes.</p>"},{"location":"security/rbac/","title":"Role-Based Access Control (RBAC)","text":"<p>RBAC restricts access to resources based on the roles of individual users within your organization. Kubernetes uses RBAC to define which users or applications can perform specific operations on a cluster.</p>"},{"location":"security/rbac/#example-rbac-for-namespaces","title":"Example: RBAC for Namespaces","text":"<p>The following example configures a <code>RoleBinding</code> that grants <code>read-only</code> access to resources in the <code>development</code> namespace:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: development\n  name: dev-read-only\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\", \"services\", \"endpoints\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-only-binding\n  namespace: development\nsubjects:\n- kind: User\n  name: dev-user\nroleRef:\n  kind: Role\n  name: dev-read-only\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"security/secrets-management/","title":"Secrets Management","text":"<p>Kubernetes provides a mechanism for managing sensitive information such as passwords, OAuth tokens, and SSH keys using Secrets. However, by default, Secrets are stored unencrypted in etcd.</p>"},{"location":"security/secrets-management/#example-using-kubernetes-secrets","title":"Example: Using Kubernetes Secrets","text":"<p>The following example demonstrates how to create and use a Kubernetes Secret for storing a database password:</p> <p>Create the Secret:</p> <pre><code>kubectl create secret generic db-password --from-literal=password=secret123\n</code></pre> <p>Reference the Secret in a Pod:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: db-pod\nspec:\n  containers:\n  - name: db-container\n    image: mysql:5.7\n    env:\n    - name: MYSQL_ROOT_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: db-password\n          key: password\n</code></pre>"},{"location":"security/serviceaccounts/","title":"Kubernetes Service Accounts","text":""},{"location":"security/serviceaccounts/#outcomes","title":"Outcomes","text":"<p>In this lab you will learn how to: - Create a service account - Use a service account to access the Kubernetes API</p>"},{"location":"security/serviceaccounts/#outline","title":"Outline","text":"<ol> <li>Create a service account</li> </ol> <pre><code>$ kubectl create serviceaccount myserviceaccount\n</code></pre> <ol> <li>Create a pod that uses the service account</li> </ol> <pre><code>$ kubectl run mypod --image=nginx --serviceaccount=myserviceaccount\n</code></pre> <ol> <li>Get the token for the service account</li> </ol> <pre><code>$ kubectl get secret $(kubectl get serviceaccount myserviceaccount -o jsonpath='{.secrets[0].name}') -o jsonpath='{.data.token}' | base64 -d\n</code></pre>"}]}